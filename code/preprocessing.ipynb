{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import xml.etree.ElementTree as etree\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "archived_xml = '../resources/training-data/WSD_Training_Corpora/SemCor/semcor.data.xml'\n",
    "mapping_file = '../resources/training-data/WSD_Training_Corpora/SemCor/semcor.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensekeyToSynsetConverter(sensekey: str):\n",
    "    '''retrieves a WordNet synset from a sensekey using the nltk package'''\n",
    "    synset = wn.lemma_from_key(sensekey).synset()\n",
    "    \n",
    "    synset_id = \"wn:\" + str(synset.offset()).zfill(8) + synset.pos()\n",
    "    return synset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>sensekey1</th>\n",
       "      <th>sensekey2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review%2:31:00::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective%1:09:00::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit%1:21:00::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_idx            sensekey1 sensekey2\n",
       "0  d000.s000.t000       long%3:00:02::       NaN\n",
       "1  d000.s000.t001         be%2:42:03::       NaN\n",
       "2  d000.s000.t002     review%2:31:00::       NaN\n",
       "3  d000.s000.t003  objective%1:09:00::       NaN\n",
       "4  d000.s000.t004    benefit%1:21:00::       NaN"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = pd.read_table(mapping_file, sep = ' ', names = ['sentence_idx', 'sensekey1', 'sensekey2'])\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from sensekey to synset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "my bar!: 100%|██████████| 226036/226036 [03:49<00:00, 983.39it/s] \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# converting from sensekey to synset id for the two columns\n",
    "mapping['sensekey1'] = mapping['sensekey1'].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# using notnull() instead of dropna because dropna() does not work on particular columns\n",
    "mapping['sensekey2'][mapping['sensekey2'].notnull()] = mapping['sensekey2'][mapping['sensekey2'].notnull()].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>sensekey1</th>\n",
       "      <th>sensekey2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>d000.s010.t003</td>\n",
       "      <td>wn:01061489a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>d000.s010.t004</td>\n",
       "      <td>wn:00081572n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>d000.s010.t005</td>\n",
       "      <td>wn:08186047n</td>\n",
       "      <td>wn:01209576n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>d000.s010.t006</td>\n",
       "      <td>wn:10053808n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>d000.s011.t000</td>\n",
       "      <td>wn:00248977n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_idx     sensekey1     sensekey2\n",
       "70  d000.s010.t003  wn:01061489a           NaN\n",
       "71  d000.s010.t004  wn:00081572n           NaN\n",
       "72  d000.s010.t005  wn:08186047n  wn:01209576n\n",
       "73  d000.s010.t006  wn:10053808n           NaN\n",
       "74  d000.s011.t000  wn:00248977n           NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping[70:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet to BabelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet2lexnames.tsv    error_idx.txt            \u001b[34mtraining-data\u001b[m\u001b[m\n",
      "babelnet2wndomains.tsv   parsed_corpora_final.txt\n",
      "babelnet2wordnet.tsv     test.xml\n"
     ]
    }
   ],
   "source": [
    "!ls ../resources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BabelNet</th>\n",
       "      <th>WordNet</th>\n",
       "      <th>WordNet2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bn:00000001n</td>\n",
       "      <td>wn:08641944n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bn:00000002n</td>\n",
       "      <td>wn:08950407n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bn:00000003n</td>\n",
       "      <td>wn:04502851n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bn:00000004n</td>\n",
       "      <td>wn:13742358n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bn:00000005n</td>\n",
       "      <td>wn:13742573n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       BabelNet       WordNet  WordNet2\n",
       "0  bn:00000001n  wn:08641944n       NaN\n",
       "1  bn:00000002n  wn:08950407n       NaN\n",
       "2  bn:00000003n  wn:04502851n       NaN\n",
       "3  bn:00000004n  wn:13742358n       NaN\n",
       "4  bn:00000005n  wn:13742573n       NaN"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../resources/babelnet2wordnet.tsv'\n",
    "BabelNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNet', 'WordNet2'])\n",
    "BabelNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BabelNet</th>\n",
       "      <th>WordNetDomain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bn:00000001n</td>\n",
       "      <td>factotum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bn:00000002n</td>\n",
       "      <td>geography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bn:00000003n</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bn:00000004n</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bn:00000005n</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       BabelNet WordNetDomain\n",
       "0  bn:00000001n      factotum\n",
       "1  bn:00000002n     geography\n",
       "2  bn:00000003n      military\n",
       "3  bn:00000004n        number\n",
       "4  bn:00000005n        number"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../resources/babelnet2wndomains.tsv'\n",
    "WordNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNetDomain'])\n",
    "WordNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BabelNet</th>\n",
       "      <th>LexNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bn:00000001n</td>\n",
       "      <td>noun.location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bn:00000002n</td>\n",
       "      <td>noun.location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bn:00000003n</td>\n",
       "      <td>noun.artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bn:00000004n</td>\n",
       "      <td>noun.quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bn:00000005n</td>\n",
       "      <td>noun.quantity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       BabelNet       LexNames\n",
       "0  bn:00000001n  noun.location\n",
       "1  bn:00000002n  noun.location\n",
       "2  bn:00000003n  noun.artifact\n",
       "3  bn:00000004n  noun.quantity\n",
       "4  bn:00000005n  noun.quantity"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../resources/babelnet2lexnames.tsv'\n",
    "LexicographerNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'LexNames'])\n",
    "LexicographerNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "7it [00:00, 63.84it/s]\u001b[A\n",
      "23it [00:00, 97.68it/s]\u001b[A\n",
      "35it [00:00, 99.07it/s]\u001b[A\n",
      "55it [00:00, 114.94it/s]\u001b[A\n",
      "67it [00:00, 111.11it/s]\u001b[A\n",
      "77it [00:00, 105.39it/s]\u001b[A\n",
      "87it [00:00, 103.34it/s]\u001b[A\n",
      "97it [00:00, 102.86it/s]\u001b[A\n",
      "109it [00:01, 102.24it/s]\u001b[A\n",
      "127it [00:01, 106.67it/s]\u001b[A\n",
      "141it [00:01, 107.67it/s]\u001b[A\n",
      "159it [00:01, 110.78it/s]\u001b[A\n",
      "175it [00:01, 112.31it/s]\u001b[A\n",
      "188it [00:01, 109.11it/s]\u001b[A\n",
      "207it [00:01, 112.25it/s]\u001b[A\n",
      "221it [00:01, 112.45it/s]\u001b[A\n",
      "237it [00:02, 113.70it/s]\u001b[A\n",
      "250it [00:02, 111.47it/s]\u001b[A\n",
      "265it [00:02, 111.71it/s]\u001b[A\n",
      "277it [00:02, 109.22it/s]\u001b[A\n",
      "293it [00:02, 110.00it/s]\u001b[A\n",
      "304it [00:02, 108.93it/s]\u001b[A\n",
      "317it [00:02, 108.48it/s]\u001b[A\n",
      "331it [00:03, 108.75it/s]\u001b[A\n",
      "342it [00:03, 106.58it/s]\u001b[A\n",
      "357it [00:03, 106.97it/s]\u001b[A\n",
      "371it [00:03, 107.01it/s]\u001b[A\n",
      "389it [00:03, 108.15it/s]\u001b[A\n",
      "407it [00:03, 109.29it/s]\u001b[A\n",
      "429it [00:03, 111.39it/s]\u001b[A\n",
      "443it [00:04, 109.25it/s]\u001b[A\n",
      "455it [00:04, 108.58it/s]\u001b[A\n",
      "471it [00:04, 108.84it/s]\u001b[A\n",
      "483it [00:04, 108.24it/s]\u001b[A\n",
      "495it [00:04, 107.88it/s]\u001b[A\n",
      "513it [00:04, 108.27it/s]\u001b[A\n",
      "524it [00:04, 107.96it/s]\u001b[A\n",
      "539it [00:04, 108.52it/s]\u001b[A\n",
      "561it [00:05, 110.36it/s]\u001b[A\n",
      "575it [00:05, 110.62it/s]\u001b[A\n",
      "591it [00:05, 111.02it/s]\u001b[A\n",
      "605it [00:05, 110.99it/s]\u001b[A\n",
      "623it [00:05, 111.84it/s]\u001b[A\n",
      "637it [00:05, 111.07it/s]\u001b[A\n",
      "649it [00:05, 110.25it/s]\u001b[A\n",
      "667it [00:06, 110.93it/s]\u001b[A\n",
      "681it [00:06, 111.08it/s]\u001b[A\n",
      "693it [00:06, 110.71it/s]\u001b[A\n",
      "705it [00:06, 110.49it/s]\u001b[A\n",
      "716it [00:06, 110.20it/s]\u001b[A\n",
      "729it [00:06, 110.22it/s]\u001b[A\n",
      "749it [00:06, 111.32it/s]\u001b[A\n",
      "767it [00:06, 112.08it/s]\u001b[A\n",
      "791it [00:06, 113.66it/s]\u001b[A\n",
      "807it [00:07, 113.47it/s]\u001b[A\n",
      "822it [00:07, 113.57it/s]\u001b[A\n",
      "my bar!:   0%|          | 1/656 [28:56<315:52:32, 1736.11s/it]\n",
      "999it [00:27, 36.35it/s] "
     ]
    }
   ],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "c=0\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'Wordnet1', 'Wordnet2', 'lemma', 'text', 'BabelNet', 'WordNetDomain', 'LexNames'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "        \n",
    "        if elem.tag == \"instance\" and event == 'end':\n",
    "            \n",
    "            instance_id = elem.get(\"id\")\n",
    "            m = mapping[mapping['sentence_idx']==instance_id]\n",
    "            lemma = elem.get(\"lemma\")\n",
    "            text = elem.text\n",
    "            \n",
    "            #get babelnet id from wordnet synset\n",
    "            BNet = BabelNet[BabelNet['WordNet'] == m['sensekey1'].iloc[0]]['BabelNet']\n",
    "            WordNetDomain = WordNet[WordNet['BabelNet'] == BNet.iloc[0]]['WordNetDomain']\n",
    "            LexNet = LexicographerNet[LexicographerNet['BabelNet'] == BNet.iloc[0]]['LexNames']\n",
    "            \n",
    "            if len(WordNetDomain)>0:\n",
    "                #print(list(m.iloc[0]), lemma, text, BNet.iloc[0], WordNetDomain.iloc[0], LexNet.iloc[0])\n",
    "                csv_writer.writerow([instance_id,\n",
    "                                    m['sensekey1'].iloc[0], m['sensekey2'].iloc[0], \n",
    "                                    lemma, text, \n",
    "                                    BNet.iloc[0], WordNetDomain.iloc[0], LexNet.iloc[0]])\n",
    "        if idx == 1000:\n",
    "            break\n",
    "    elem.clear()\n",
    "del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wn:02604760v'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['sensekey1'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_idx    d000.s002.t000\n",
       "sensekey1         wn:00786195n\n",
       "sensekey2                  NaN\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62540    factotum\n",
       "Name: WordNetDomain, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensekeyToSynsetConverter(m['sensekey1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensekeyToSynsetConverter(m['sensekey2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BabelNet</th>\n",
       "      <th>LexNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64641</th>\n",
       "      <td>bn:00064646n</td>\n",
       "      <td>noun.cognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           BabelNet        LexNames\n",
       "64641  bn:00064646n  noun.cognition"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " LexicographerNet[LexicographerNet['BabelNet'] == BNet.iloc[0]]['LexNames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train =  []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "#reading the gold.txt.file for getting the word and sense for specific id \n",
    "\n",
    "input_file =  open(\"inputFile.csv\",'a')\n",
    "csv_writer =  csv.writer(input_file)\n",
    "csv_writer.writerow(('Input', 'label'))\n",
    "\n",
    "\n",
    "dictionary = {}\n",
    "with open(mapping_file) as fp:\n",
    "    \n",
    "    for line in fp:\n",
    "        data  =  line.split()\n",
    "        dictionary[data[0]] = data[1]\n",
    "        \n",
    "\n",
    "for start,element  in ET.iterparse(archived_xml, events=(\"start\",'end')):\n",
    "    if element.tag == 'instance' and start == 'start':\n",
    "        \n",
    "        insta_id = element.get(\"id\")\n",
    "\n",
    "\n",
    "        if dictionary.get(insta_id) != None:\n",
    "            x.append(element.text)\n",
    "            y.append(dictionary.get(insta_id))\n",
    "\n",
    "    elif element.tag =='sentence' and start=='end':\n",
    "       \n",
    "        if len(x) > 0:\n",
    "            for i in range(len(x)):\n",
    "                myList = [x[i],y[i]]\n",
    "                csv_writer.writerow(myList)\n",
    "            \n",
    "    \n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "    \n",
    "    element.clear()\n",
    "    # Also eliminate now-empty references from the root node to elem\n",
    "    for ancestor in element.findall('.//key'):\n",
    "        while ancestor.getprevious() is not None:\n",
    "            del ancestor.getparent()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions:\n",
    "1. babelnet\n",
    "2. wordnet_domains\n",
    "3. lexicographer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BabelNet synset ids to WordNet offset ids\n",
    "synset_id = pd.read_csv(mapping_file, sep = \"\\t\", error_bad_lines=False, header = None)\n",
    "synset_id.columns = ['BabelNet', 'WordNet']\n",
    "BabelNet_id = list(synset_id['BabelNet'])\n",
    "\n",
    "\n",
    "N_annotations_present, textExists = False, False\n",
    "bigrams, unigrams = [], []\n",
    "sentence = ''\n",
    "#total = 153763675\n",
    "total = 290000000\n",
    "#Write to file the last sentence written so in case of a crash, you can write and process starting from that sentence\n",
    "\n",
    "f = open(\"../resources/parsed_corpora_RECOVERY_final.txt\", \"r+\")\n",
    "annotations_per_sentence = open(\"../resources/parsed_corpora_annotations_final.txt\", \"a\")\n",
    "data = f.readlines()\n",
    "#checks for last iteration if exists\n",
    "last_iteration, written_lines = [int(i) for i in data[-1].split(\",\")]\n",
    "print(\"starting processing from iteration # {}\\t Written lines so far: {}\".format(last_iteration, written_lines))\n",
    "\n",
    "############################\n",
    "## main iteration start ###\n",
    "###########################\n",
    "with open('../resources/parsed_corpora_final.txt', 'a', encoding='utf-8') as file:\n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "\n",
    "        #checks current idx so if preprocessing crashes, It start processing from this iteration\n",
    "        if last_iteration < idx:\n",
    "            #taking start of each sentence\n",
    "            if elem.tag=='sentence' and event == 'start':\n",
    "                sentence_idx = elem.get(\"id\")\n",
    "\n",
    "            #taking the sentence text (English only)\n",
    "            if elem.tag == 'text' and elem.get(\"lang\") == 'en':\n",
    "                #checking if the text is not a None\n",
    "                if elem.text!= None:\n",
    "                    sentence += elem.text\n",
    "                    textExists =  True\n",
    "                else:\n",
    "                    textExists = False\n",
    "\n",
    "            if textExists:\n",
    "                #taking the start of the sentence annotations (English only)\n",
    "                if elem.tag == 'annotation' and elem.get(\"lang\") == 'en' and event == 'start':\n",
    "                    # checking if annotation is in the mapping from BabelNet to WordNet\n",
    "\n",
    "                    current_synset_id = elem.text\n",
    "                    if current_synset_id in BabelNet_id:\n",
    "                        N_annotations_present=True\n",
    "                        anchor = elem.get(\"anchor\")\n",
    "                        replace_by = \"_\".join(elem.get(\"lemma\").split(\" \")) + \"_\" + elem.text\n",
    "\n",
    "                        #write N-grams and unigrams into memory\n",
    "                        if len(anchor.split(\" \"))>1:\n",
    "                            bigrams.append([anchor, replace_by])\n",
    "                        elif len(anchor.split(\" \"))==1:\n",
    "                            unigrams.append([anchor, replace_by])\n",
    "\n",
    "                #after iterating through all annotations, write the transformed sentence\n",
    "                if elem.tag=='sentence' and event == 'end':\n",
    "                    if N_annotations_present:\n",
    "                        annotations = 0\n",
    "\n",
    "                        #ensure longest n-grams dominate, then replace\n",
    "                        bigrams = sorted(bigrams, key = lambda k: len(k[0].split(\" \")), reverse = True)\n",
    "\n",
    "                        #replace n-grams\n",
    "                        sentence = sentence.replace(\"-\",\" \")\n",
    "                        for orig, replace in bigrams:\n",
    "                            annotations+=1\n",
    "                            wrap = lambda x: \" \"+x+\" \"\n",
    "                            sentence = sentence.replace(wrap(orig), wrap(replace))\n",
    "\n",
    "                        #split before unigrams so nothing gets replaced twice\n",
    "                        sentence = sentence.split(\" \")\n",
    "\n",
    "                        #UNIGRAMS replacement\n",
    "                        for index, (orig, replace) in enumerate(unigrams):\n",
    "                            if orig in sentence:\n",
    "                                annotations+=1\n",
    "                                sentence[sentence.index(orig)] = replace\n",
    "\n",
    "                        #join back to write to file\n",
    "                        sentence = \" \".join(sentence)\n",
    "\n",
    "                        #write to file\n",
    "                        file.write(sentence+\"\\n\")\n",
    "                        annotations_per_sentence.writelines(str(annotations)+\"\\n\")\n",
    "                        written_lines+=1\n",
    "\n",
    "                        #reset\n",
    "                        bigrams, unigrams, N_annotations_present, sentence = [], [], False, ''\n",
    "\n",
    "                    else:\n",
    "                        sentence = ''\n",
    "                    f.writelines(\"{},{}\\n\".format(str(idx), str(written_lines)))\n",
    "        #debugging\n",
    "        if (idx+1)%5000000==0:\n",
    "            print(\"Number of actually written lines: {}\\n {:.3f}% done\".format(written_lines, ((idx+last_iteration)/total)*100))\n",
    "            #break\n",
    "            #delete to ease memory\n",
    "        elem.clear()\n",
    "    del context\n",
    "\n",
    "##########################\n",
    "## main iteration end ###\n",
    "#########################\n",
    "print(\"_\"*120)\n",
    "print(\"Number of actually written lines: {}\".format(written_lines))\n",
    "\n",
    "f.close()\n",
    "annotations_per_sentence.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(23+26+30.5)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
