{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import xml.etree.ElementTree as etree\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "archived_xml = '../resources/training-data/WSD_Training_Corpora/SemCor/semcor.data.xml'\n",
    "mapping_file = '../resources/training-data/WSD_Training_Corpora/SemCor/semcor.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensekeyToSynsetConverter(sensekey: str):\n",
    "    '''retrieves a WordNet synset from a sensekey using the nltk package'''\n",
    "    synset = wn.lemma_from_key(sensekey).synset()\n",
    "    \n",
    "    synset_id = \"wn:\" + str(synset.offset()).zfill(8) + synset.pos()\n",
    "    return synset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>sensekey1</th>\n",
       "      <th>sensekey2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review%2:31:00::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective%1:09:00::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit%1:21:00::</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_idx            sensekey1 sensekey2\n",
       "0  d000.s000.t000       long%3:00:02::       NaN\n",
       "1  d000.s000.t001         be%2:42:03::       NaN\n",
       "2  d000.s000.t002     review%2:31:00::       NaN\n",
       "3  d000.s000.t003  objective%1:09:00::       NaN\n",
       "4  d000.s000.t004    benefit%1:21:00::       NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = pd.read_table(mapping_file, sep = ' ', names = ['sentence_idx', 'sensekey1', 'sensekey2'])\n",
    "mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mapping['sensekey2'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from sensekey to synset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# converting from sensekey to synset id for the two columns\n",
    "mapping['sensekey1'] = mapping['sensekey1'].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# using notnull() instead of dropna because dropna() does not work on particular columns\n",
    "mapping['sensekey2'][mapping['sensekey2'].notnull()] = mapping['sensekey2'][mapping['sensekey2'].notnull()].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet to BabelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2wordnet.tsv'\n",
    "BabelNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNet', 'WordNet2'])\n",
    "BabelNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2wndomains.tsv'\n",
    "WordNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNetDomain'])\n",
    "WordNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2lexnames.tsv'\n",
    "LexicographerNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'LexNames'])\n",
    "LexicographerNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "32it [00:00, 295.16it/s]\u001b[A\n",
      "58it [00:00, 278.03it/s]\u001b[A\n",
      "88it [00:00, 273.77it/s]\u001b[A\n",
      "116it [00:00, 273.19it/s]\u001b[A\n",
      "158it [00:00, 293.47it/s]\u001b[A\n",
      "186it [00:00, 285.49it/s]\u001b[A\n",
      "220it [00:00, 289.54it/s]\u001b[A\n",
      "248it [00:00, 275.95it/s]\u001b[A\n",
      "274it [00:00, 274.54it/s]\u001b[A\n",
      "302it [00:01, 271.59it/s]\u001b[A\n",
      "330it [00:01, 271.75it/s]\u001b[A\n",
      "356it [00:01, 266.33it/s]\u001b[A\n",
      "388it [00:01, 269.19it/s]\u001b[A\n",
      "430it [00:01, 276.54it/s]\u001b[A\n",
      "460it [00:01, 270.18it/s]\u001b[A\n",
      "494it [00:01, 272.07it/s]\u001b[A\n",
      "530it [00:01, 275.04it/s]\u001b[A\n",
      "566it [00:02, 277.59it/s]\u001b[A\n",
      "597it [00:02, 278.98it/s]\u001b[A\n",
      "628it [00:02, 280.28it/s]\u001b[A\n",
      "659it [00:02, 278.65it/s]\u001b[A\n",
      "688it [00:02, 275.56it/s]\u001b[A\n",
      "715it [00:02, 273.61it/s]\u001b[A\n",
      "758it [00:02, 277.91it/s]\u001b[A\n",
      "996it [00:03, 270.66it/s]"
     ]
    }
   ],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'X', 'y'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "            idx = elem.get(\"id\")\n",
    "            X, y = [], []\n",
    "\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            word = elem.text\n",
    "            X.append(word)\n",
    "            y.append(word)\n",
    "\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            # create dict {lemma: [sensekey1, sensekey2]}\n",
    "            word = elem.text\n",
    "            X.append(word)\n",
    "\n",
    "            #get sensekeys from mapping row\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            #get rid of nan's if there is only one sensekey instead of two\n",
    "            cleanedList = [x for x in l if str(x) != 'nan']\n",
    "            y.append(cleanedList)\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            #to_dump = {'x':X, 'y':y}\n",
    "            csv_writer.writerow([idx, X, y])\n",
    "        if idx==1000:\n",
    "            break\n",
    "        elem.clear()\n",
    "del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../resources/f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['How', ['long%3:00:02::'], 'has', 'it', ['be%2:42:03::'], 'since', 'you', ['review%2:31:00::'], 'the', ['objective%1:09:00::'], 'of', 'your', ['benefit%1:21:00::'], 'and', ['service%1:04:07::'], ['program%1:09:01::'], '?']\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How',\n",
       " ['long%3:00:02::'],\n",
       " 'has',\n",
       " 'it',\n",
       " ['be%2:42:03::'],\n",
       " 'since',\n",
       " 'you',\n",
       " ['review%2:31:00::'],\n",
       " 'the',\n",
       " ['objective%1:09:00::'],\n",
       " 'of',\n",
       " 'your',\n",
       " ['benefit%1:21:00::'],\n",
       " 'and',\n",
       " ['service%1:04:07::'],\n",
       " ['program%1:09:01::'],\n",
       " '?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions:\n",
    "1. babelnet\n",
    "2. wordnet_domains\n",
    "3. lexicographer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': ['How',\n",
       "  'long',\n",
       "  'has',\n",
       "  'it',\n",
       "  'been',\n",
       "  'since',\n",
       "  'you',\n",
       "  'reviewed',\n",
       "  'the',\n",
       "  'objectives',\n",
       "  'of',\n",
       "  'your',\n",
       "  'benefit',\n",
       "  'and',\n",
       "  'service',\n",
       "  'program',\n",
       "  '?']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNet.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNet.iloc[0] in WordNet['BabelNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.iloc[0][1:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensekeys.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    \n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'sensekey1', 'sensekey2', 'lemma', 'text'))#, 'BabelNet', 'WordNetDomain', 'LexNames'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "\n",
    "            sentence, y = []\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            word = elem.text\n",
    "            sentence.append(word)\n",
    "            y.append(word)\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            # create dict {lemma: [sensekey1, sensekey2]}\n",
    "            word = elem.text\n",
    "            sentence.append(word)\n",
    "            #sensekeys = m.drop(columns=[\"sentence_idx\"]).dropna(axis=1)\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            cleanedList = [x for x in l if str(x) != 'nan']\n",
    "            y.append(cleanedList)\n",
    "#             csv_writer.writerow([instance_id,\n",
    "#                                 m['sensekey1'].iloc[0], m['sensekey2'].iloc[0], \n",
    "#                                 lemma, text])\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            \n",
    "            print(sentence)\n",
    "            print(y)\n",
    "            \n",
    "\n",
    "            #get babelnet id from wordnet synset\n",
    "#             BNet = BabelNet[BabelNet['WordNet'] == m['sensekey1'].iloc[0]]['BabelNet']\n",
    "#             WordNetDomain = WordNet[WordNet['BabelNet'] == BNet.iloc[0]]['WordNetDomain']\n",
    "#             LexNet = LexicographerNet[LexicographerNet['BabelNet'] == BNet.iloc[0]]['LexNames']\n",
    "#             print(list(m.iloc[0]), lemma, text, BNet.iloc[0], WordNetDomain, LexNet.iloc[0])\n",
    "#             csv_writer.writerow([instance_id,\n",
    "#                                 m['sensekey1'].iloc[0], m['sensekey2'].iloc[0], \n",
    "#                                 lemma, text, \n",
    "#                                 BNet.iloc[0], WordNetDomain.iloc[0], LexNet.iloc[0]])\n",
    "        elem.clear()\n",
    "del context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
