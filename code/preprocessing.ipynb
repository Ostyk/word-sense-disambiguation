{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import xml.etree.ElementTree as etree\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "archived_xml = '../resources/training-data/WSD_Training_Corpora/SemCor/semcor.data.xml'\n",
    "mapping_file = '../resources/training-data/WSD_Training_Corpora/SemCor/semcor.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_table(mapping_file, sep = ' ', names = ['sentence_idx', 'sensekey1', 'sensekey2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10036it [00:35, 281.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1394845823713222% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16172it [00:56, 285.39it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0f9c96e6d06f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"instance\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'start'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# get mapping from idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16172it [01:10, 230.84it/s]"
     ]
    }
   ],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    \n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'X', 'y','sensekeyCount'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "        \n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "            sentence_id = elem.get(\"id\")\n",
    "            X, y, senseCount = [], [], 0\n",
    "\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            X.append(elem.text)\n",
    "            y.append(elem.text)\n",
    "\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            X.append(elem.text)\n",
    "\n",
    "            #get sensekeys from mapping row\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            cleanedList = [x for x in l if str(x) != 'nan'] #gets rid of NaN's\n",
    "            senseCount += len(cleanedList)\n",
    "            y.append(cleanedList)\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            csv_writer.writerow([sentence_id, X, y, senseCount])\n",
    "\n",
    "        if (idx+1)%10000==0:\n",
    "            print(\"{:1f}% complete\".format((idx/877502)*100))\n",
    "\n",
    "        elem.clear()\n",
    "del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../resources/f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1226aa470>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAEIpJREFUeJzt3X+MHGd9x/H3t/kBUQ7FMUlWlhPlQomoorgEfE2DQOguFJQmqE6lKAJFyJFSndQCoiKIHK1agVTUUKlAK1WtUkJjlR+XQKGJbGhInXNRpTZgkx92SNOE1KixHCyKnXJUojV8+8fOmcXc3ezO7d7sPbxf0ulm5mZ2P37u/PH42Z25yEwkSRvfL7QdQJI0HBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRBnrueTXXDBBTk5Odno2B/84Aece+65ww00JGZrxmzNmK2ZjZztwIED383MC2sfKDPX7WP79u3Z1MLCQuNjR81szZitGbM1s5GzAfuzj451ykWSCmGhS1IhLHRJKoSFLkmFsNAlqRB9vW0xIg4D3wd+BJzMzKmI2AzcC0wCh4GbM/P4aGJKkuoMcoY+k5lXZeZUtT4H7M3My4G91bokqSVrmXLZAeyqlncBN649jiSpqX4LPYGvRMSBiJittnUy82i1/ALQGXo6SVLfIvv4JdERsTUzj0TERcBDwLuBBzJzU88+xzPz/GWOnQVmATqdzvb5+flGQRcXF5mYmGh07Kitlu3gkRdPLW/bet56RTplo45b28zWjNmaqcs2MzNzoGe6e2X9XE7a+wF8EHgf8DSwpdq2BXi67tifx0v/L71j96mPNmzUcWub2ZoxWzPrdul/RJwbES9bWgbeAhwCHgB2VrvtBO6v/ddDkjQy/bxtsQN8MSKW9v9MZv5DRHwduC8ibgO+Ddw8upiSpDq1hZ6ZzwGvXmb7fwFvGkUoSdLgvFJUkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFqP0l0VrZ5NweAG7fdpLpdqNIkmfoklQKC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSIfou9Ig4IyIejYjd1fplEfFIRDwbEfdGxNmjiylJqjPIGfp7gKd61j8CfCwzXwkcB24bZjBJ0mD6KvSIuBi4AfhEtR7AtcDnq112ATeOIqAkqT/9nqF/HHg/8ONq/eXAicw8Wa0/D2wdcjZJ0gAiM1ffIeKtwPWZ+TsRMQ28D7gV+NdquoWIuAT4cmZeuczxs8AsQKfT2T4/P98o6OLiIhMTE42OHZWDR14EoHMOXLT5vFX3Adi2dfl9Rmkcx22J2ZoxWzMbOdvMzMyBzJyqfaDMXPUD+GO6Z+CHgReA/wE+DXwXOLPa53XAg3WPtX379mxqYWGh8bGjcukdu/PSO3bnn3/q72v3ufSO3euY7CfGcdyWmK0ZszWzkbMB+7OmXzOzfsolMz+QmRdn5iTwNuDhzLwFWABuqnbbCdxf+6+HJGlk1vI+9DuA90bEs3Tn1O8eTiRJUhMD/U7RzNwH7KuWnwOuHn4kSVITXikqSYWw0CWpEBa6JBVioDl0Dc/k3J5Ty4fvvKHFJJJK4Rm6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgrhzbnGmDfwkjQIz9AlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCeC+XDW6l+70sbb9920mm1zuUpFZ4hi5JhbDQJakQFrokFcJCl6RC1BZ6RLw0Ir4WEY9HxJMR8aFq+2UR8UhEPBsR90bE2aOPK0laST9n6D8Ers3MVwNXAddFxDXAR4CPZeYrgePAbaOLKUmqU1vo2bVYrZ5VfSRwLfD5avsu4MaRJJQk9aWvOfSIOCMiHgOOAQ8B3wJOZObJapfnga2jiShJ6kdkZv87R2wCvgj8AXBPNd1CRFwCfDkzr1zmmFlgFqDT6Wyfn59vFHRxcZGJiYlGx47KwSMvAtA5By7afN6q+wBs23pe7fZ+jh3k8VfL1rZx/J4uMVszZmumLtvMzMyBzJyqe5yBrhTNzBMRsQC8DtgUEWdWZ+kXA0dWOOYu4C6AqampnJ6eHuQpT9m3bx9Njx2VW3uuxrx5hWy39l7Ject07fZ+jh3k8VfL1rZx/J4uMVszZmtmWNn6eZfLhdWZORFxDvBm4ClgAbip2m0ncP+a00iSGuvnDH0LsCsizqD7D8B9mbk7Ir4JzEfEHwGPAnePMKckqUZtoWfmE8Brltn+HHD1KEKtl5VubLXSPqvtJ0lt80pRSSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSIc5sO4DaMzm359Ty4TtvaDGJpGHwDF2SCmGhS1IhLHRJKoRz6PoZzq1LG5Nn6JJUCAtdkgphoUtSIWoLPSIuiYiFiPhmRDwZEe+ptm+OiIci4pnq8/mjjytJWkk/Z+gngdsz8wrgGuCdEXEFMAfszczLgb3VuiSpJbWFnplHM/Mb1fL3gaeArcAOYFe12y7gxlGFlCTVG2gOPSImgdcAjwCdzDxafekFoDPUZJKkgURm9rdjxATwT8CHM/MLEXEiMzf1fP14Zv7MPHpEzAKzAJ1OZ/v8/HyjoIuLi0xMTDQ6diUHj7x4annb1vNq9zl9v6Wvdc6BizbXH7/csf0+96D7jDrbMIziezosZmvGbM3UZZuZmTmQmVN1j9NXoUfEWcBu4MHM/Gi17WlgOjOPRsQWYF9mvmq1x5mamsr9+/fXPt9y9u3bx/T0dKNjV9LPBTS9+5y+39LXbt92knffsmOg5xj0uQfdZ9TZhmEU39NhMVszZmumLltE9FXo/bzLJYC7gaeWyrzyALCzWt4J3F/3WJKk0enn0v/XA+8ADkbEY9W23wPuBO6LiNuAbwM3jyaiJKkftYWemf8MxApfftNw44yG9yaR9PPAK0UlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVIh+7rYo/YzV7hMvqR2eoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmF8HeKauh6f9+ov2tUWj+1Z+gR8cmIOBYRh3q2bY6IhyLimerz+aONKUmq08+Uyz3AdadtmwP2ZublwN5qXZLUotpCz8yvAt87bfMOYFe1vAu4cci5JEkDisys3yliEtidmVdW6ycyc1O1HMDxpfVljp0FZgE6nc72+fn5RkEXFxeZmJhodOzBIy+eWt629bza7Ssdu9LxnXPgos31x6/luQfdZz2zNTke1vY9HTWzNWO2ZuqyzczMHMjMqbrHWXOhV+vHM7N2Hn1qair3799f+3zL2bdvH9PT042OXelFun5evOvdZ6Xjb992knffsmOkzz3oPuuZrcnxsLbv6aiZrRmzNVOXLSL6KvSmb1v8TkRsqZ5oC3Cs4eNIkoakaaE/AOyslncC9w8njiSpqX7etvhZ4F+AV0XE8xFxG3An8OaIeAb4tWpdktSi2guLMvPtK3zpTUPOIklaAy/9l6RCWOiSVAgLXZIKseFvzuWNoCSpyzN0SSqEhS5JhbDQJakQG34OXeXy9RFpMJ6hS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhfBeLmrF5Nwebt92klvn9gx8n5Z+7vGy0j7eH0Yl8wxdkgphoUtSISx0SSrEhin0g0deZHJuz0/NgUqjMDm359TPm7SRbJhClyStzkKXpEJY6JJUCAtdkgrhhUXSGFnLhU9eNCXP0CWpEBa6JBXCQpekQqxpDj0irgP+DDgD+ERm3jmUVNIYGvSmYKfvt9HnuNcj/0Yfo15t/Fkan6FHxBnAXwC/DlwBvD0irhhWMEnSYNYy5XI18GxmPpeZ/wvMAzuGE0uSNKi1FPpW4D971p+vtkmSWhCZ2ezAiJuA6zLzt6r1dwC/mpnvOm2/WWC2Wn0V8HTDrBcA32147KiZrRmzNWO2ZjZytksz88K6B1nLi6JHgEt61i+utv2UzLwLuGsNzwNAROzPzKm1Ps4omK0ZszVjtmZ+HrKtZcrl68DlEXFZRJwNvA14YK2BJEnNND5Dz8yTEfEu4EG6b1v8ZGY+ObRkkqSBrOl96Jn5JeBLQ8pSZ83TNiNktmbM1ozZmik+W+MXRSVJ48VL/yWpEBui0CPiuoh4OiKejYi5tvP0iojDEXEwIh6LiP0tZ/lkRByLiEM92zZHxEMR8Uz1+fwxyvbBiDhSjd1jEXF9S9kuiYiFiPhmRDwZEe+ptrc+dqtka33sIuKlEfG1iHi8yvahavtlEfFI9ff13upNE+OS7Z6I+I+ecbtqvbNVOc6IiEcjYne1Ppwxy8yx/qD7guu3gFcAZwOPA1e0nasn32HggrZzVFneCLwWONSz7U+AuWp5DvjIGGX7IPC+MRi3LcBrq+WXAf9O93YWrY/dKtlaHzsggIlq+SzgEeAa4D7gbdX2vwJ+e4yy3QPcNAY/c+8FPgPsrtaHMmYb4QzdWwz0KTO/CnzvtM07gF3V8i7gxnUNVVkh21jIzKOZ+Y1q+fvAU3Svem597FbJ1rrsWqxWz6o+ErgW+Hy1va1xWylb6yLiYuAG4BPVejCkMdsIhT7utxhI4CsRcaC6KnbcdDLzaLX8AtBpM8wy3hURT1RTMq1MB/WKiEngNXTP6MZq7E7LBmMwdtXUwWPAMeAhuv+bPpGZJ6tdWvv7enq2zFwatw9X4/axiHhJC9E+Drwf+HG1/nKGNGYbodDH3Rsy87V07zr5zoh4Y9uBVpLd/8+NxVlK5S+BXwSuAo4Cf9pmmIiYAP4O+N3M/O/er7U9dstkG4uxy8wfZeZVdK8Uvxr4pTZyLOf0bBFxJfABuhl/BdgM3LGemSLircCxzDwwisffCIXe1y0G2pKZR6rPx4Av0v2hHiffiYgtANXnYy3nOSUzv1P9pfsx8Ne0OHYRcRbdwvx0Zn6h2jwWY7dctnEauyrPCWABeB2wKSKWrnFp/e9rT7brqimszMwfAn/D+o/b64HfiIjDdKePr6X7OyWGMmYbodDH9hYDEXFuRLxsaRl4C3Bo9aPW3QPAzmp5J3B/i1l+ylJZVn6TlsaumsO8G3gqMz/a86XWx26lbOMwdhFxYURsqpbPAd5Md45/Abip2q2tcVsu27/1/AMddOep13XcMvMDmXlxZk7S7bKHM/MWhjVmbb/a2+crwtfTfXX/W8Dvt52nJ9cr6L7r5nHgybazAZ+l+9/v/6M7D3cb3fm5vcAzwD8Cm8co298CB4En6JbnlpayvYHudMoTwGPVx/XjMHarZGt97IBfBh6tMhwC/rDa/grga8CzwOeAl4xRtoercTsEfIrqnTAt/dxN85N3uQxlzLxSVJIKsRGmXCRJfbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqxP8DgltQswrfZs0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['sensekeyCount'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensekeyToSynsetConverter(sensekey: str):\n",
    "    '''retrieves a WordNet synset from a sensekey using the nltk package'''\n",
    "    synset = wn.lemma_from_key(sensekey).synset()\n",
    "    \n",
    "    synset_id = \"wn:\" + str(synset.offset()).zfill(8) + synset.pos()\n",
    "    return synset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from sensekey to synset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# converting from sensekey to synset id for the two columns\n",
    "mapping['sensekey1'] = mapping['sensekey1'].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# using notnull() instead of dropna because dropna() does not work on particular columns\n",
    "mapping['sensekey2'][mapping['sensekey2'].notnull()] = mapping['sensekey2'][mapping['sensekey2'].notnull()].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet to BabelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2wordnet.tsv'\n",
    "BabelNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNet', 'WordNet2'])\n",
    "BabelNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2wndomains.tsv'\n",
    "WordNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNetDomain'])\n",
    "WordNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2lexnames.tsv'\n",
    "LexicographerNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'LexNames'])\n",
    "LexicographerNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'X', 'y','sensekeyCount'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "            sentence_id = elem.get(\"id\")\n",
    "            X, y, senseCount = [], [], []\n",
    "\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            word = elem.text\n",
    "            X.append(word)\n",
    "            y.append(word)\n",
    "\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            # create dict {lemma: [sensekey1, sensekey2]}\n",
    "            word = elem.text\n",
    "            X.append(word)\n",
    "\n",
    "            #get sensekeys from mapping row\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            #get rid of nan's if there is only one sensekey instead of two\n",
    "            cleanedList = [x for x in l if str(x) != 'nan']\n",
    "            senseCount.append(len(cleanedList))\n",
    "            y.append(cleanedList)\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            #to_dump = {'x':X, 'y':y}\n",
    "            csv_writer.writerow([sentence_id, X, y, max(senseCount)])\n",
    "        if idx==200:\n",
    "            break\n",
    "        elem.clear()\n",
    "del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(senseCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senseCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../resources/f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(senseCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions:\n",
    "1. babelnet\n",
    "2. wordnet_domains\n",
    "3. lexicographer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNet.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNet.iloc[0] in WordNet['BabelNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.iloc[0][1:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensekeys.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    \n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'sensekey1', 'sensekey2', 'lemma', 'text'))#, 'BabelNet', 'WordNetDomain', 'LexNames'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "\n",
    "            sentence, y = []\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            word = elem.text\n",
    "            sentence.append(word)\n",
    "            y.append(word)\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            # create dict {lemma: [sensekey1, sensekey2]}\n",
    "            word = elem.text\n",
    "            sentence.append(word)\n",
    "            #sensekeys = m.drop(columns=[\"sentence_idx\"]).dropna(axis=1)\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            cleanedList = [x for x in l if str(x) != 'nan']\n",
    "            y.append(cleanedList)\n",
    "#             csv_writer.writerow([instance_id,\n",
    "#                                 m['sensekey1'].iloc[0], m['sensekey2'].iloc[0], \n",
    "#                                 lemma, text])\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            \n",
    "            print(sentence)\n",
    "            print(y)\n",
    "            \n",
    "\n",
    "            #get babelnet id from wordnet synset\n",
    "#             BNet = BabelNet[BabelNet['WordNet'] == m['sensekey1'].iloc[0]]['BabelNet']\n",
    "#             WordNetDomain = WordNet[WordNet['BabelNet'] == BNet.iloc[0]]['WordNetDomain']\n",
    "#             LexNet = LexicographerNet[LexicographerNet['BabelNet'] == BNet.iloc[0]]['LexNames']\n",
    "#             print(list(m.iloc[0]), lemma, text, BNet.iloc[0], WordNetDomain, LexNet.iloc[0])\n",
    "#             csv_writer.writerow([instance_id,\n",
    "#                                 m['sensekey1'].iloc[0], m['sensekey2'].iloc[0], \n",
    "#                                 lemma, text, \n",
    "#                                 BNet.iloc[0], WordNetDomain.iloc[0], LexNet.iloc[0]])\n",
    "        elem.clear()\n",
    "del context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
