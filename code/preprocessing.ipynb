{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import xml.etree.ElementTree as etree\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archived_xml = '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor+OMSTI/semcor+omsti.data.xml'\n",
    "mapping_file = '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor+OMSTI/semcor+omsti.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_table(mapping_file, sep = ' ', names = ['sentence_idx', 'sensekey1', 'sensekey2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "\n",
    "with open('../resources/parsed_corpora.csv', 'w', encoding='utf-8') as file:\n",
    "    \n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'X', 'y','sensekeyCount'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "        \n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "            sentence_id = elem.get(\"id\")\n",
    "            X, y, senseCount = [], [], 0\n",
    "\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            X.append(elem.text)\n",
    "            y.append(elem.text)\n",
    "\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            X.append(elem.text)\n",
    "\n",
    "            #get sensekeys from mapping row\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            cleanedList = [x for x in l if str(x) != 'nan'] #gets rid of NaN's\n",
    "            senseCount += len(cleanedList)\n",
    "            y.append(cleanedList)\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            csv_writer.writerow([sentence_id, X, y, senseCount])\n",
    "\n",
    "        if (idx+1)%10**6==0:\n",
    "            print(\"{:2f}% complete\".format((idx/32946488)*100))\n",
    "\n",
    "        elem.clear()\n",
    "del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../resources/parsed_corpora.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['sensekeyCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.hist(bins=20, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensekeyToSynsetConverter(sensekey: str):\n",
    "    '''retrieves a WordNet synset from a sensekey using the nltk package'''\n",
    "    synset = wn.lemma_from_key(sensekey).synset()\n",
    "    \n",
    "    synset_id = \"wn:\" + str(synset.offset()).zfill(8) + synset.pos()\n",
    "    return synset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from sensekey to synset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# converting from sensekey to synset id for the two columns\n",
    "mapping['sensekey1'] = mapping['sensekey1'].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"my bar!\")\n",
    "# using notnull() instead of dropna because dropna() does not work on particular columns\n",
    "mapping['sensekey2'][mapping['sensekey2'].notnull()] = mapping['sensekey2'][mapping['sensekey2'].notnull()].progress_apply(sensekeyToSynsetConverter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet to BabelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2wordnet.tsv'\n",
    "BabelNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNet', 'WordNet2'])\n",
    "BabelNet.head()\n",
    "\n",
    "file = '../resources/babelnet2wndomains.tsv'\n",
    "WordNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNetDomain'])\n",
    "WordNet.head()\n",
    "\n",
    "file = '../resources/babelnet2lexnames.tsv'\n",
    "LexicographerNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'LexNames'])\n",
    "LexicographerNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2wndomains.tsv'\n",
    "WordNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'WordNetDomain'])\n",
    "WordNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../resources/babelnet2lexnames.tsv'\n",
    "LexicographerNet = pd.read_table(file, sep = '\\t', names = ['BabelNet', 'LexNames'])\n",
    "LexicographerNet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'X', 'y','sensekeyCount'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "            sentence_id = elem.get(\"id\")\n",
    "            X, y, senseCount = [], [], []\n",
    "\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            word = elem.text\n",
    "            X.append(word)\n",
    "            y.append(word)\n",
    "\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            # create dict {lemma: [sensekey1, sensekey2]}\n",
    "            word = elem.text\n",
    "            X.append(word)\n",
    "\n",
    "            #get sensekeys from mapping row\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            #get rid of nan's if there is only one sensekey instead of two\n",
    "            cleanedList = [x for x in l if str(x) != 'nan']\n",
    "            senseCount.append(len(cleanedList))\n",
    "            y.append(cleanedList)\n",
    "\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            #to_dump = {'x':X, 'y':y}\n",
    "            csv_writer.writerow([sentence_id, X, y, max(senseCount)])\n",
    "        if idx==200:\n",
    "            break\n",
    "        elem.clear()\n",
    "del context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(senseCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senseCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../resources/f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(senseCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions:\n",
    "1. babelnet\n",
    "2. wordnet_domains\n",
    "3. lexicographer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNet.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNet.iloc[0] in WordNet['BabelNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.iloc[0][1:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensekeys.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = etree.iterparse(archived_xml, events=(\"start\", \"end\"))\n",
    "\n",
    "with open('../resources/f.csv', 'w', encoding='utf-8') as file:\n",
    "    \n",
    "    csv_writer =  csv.writer(file)\n",
    "    csv_writer.writerow(('id', 'sensekey1', 'sensekey2', 'lemma', 'text'))#, 'BabelNet', 'WordNetDomain', 'LexNames'))\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(tqdm(context)):\n",
    "        if elem.tag == 'sentence' and event == 'start':\n",
    "\n",
    "            sentence, y = []\n",
    "        if elem.tag == \"wf\" and event == 'start':\n",
    "            word = elem.text\n",
    "            sentence.append(word)\n",
    "            y.append(word)\n",
    "        if elem.tag == \"instance\" and event == 'start':\n",
    "            # get mapping from idx\n",
    "            m = mapping[mapping['sentence_idx']== elem.get(\"id\")]\n",
    "            # create dict {lemma: [sensekey1, sensekey2]}\n",
    "            word = elem.text\n",
    "            sentence.append(word)\n",
    "            #sensekeys = m.drop(columns=[\"sentence_idx\"]).dropna(axis=1)\n",
    "            l = [m['sensekey1'].iloc[0], m['sensekey2'].iloc[0]]\n",
    "            cleanedList = [x for x in l if str(x) != 'nan']\n",
    "            y.append(cleanedList)\n",
    "#             csv_writer.writerow([instance_id,\n",
    "#                                 m['sensekey1'].iloc[0], m['sensekey2'].iloc[0], \n",
    "#                                 lemma, text])\n",
    "        if elem.tag == 'sentence' and event == 'end':\n",
    "            \n",
    "            print(sentence)\n",
    "            print(y)\n",
    "            \n",
    "\n",
    "            #get babelnet id from wordnet synset\n",
    "#             BNet = BabelNet[BabelNet['WordNet'] == m['sensekey1'].iloc[0]]['BabelNet']\n",
    "#             WordNetDomain = WordNet[WordNet['BabelNet'] == BNet.iloc[0]]['WordNetDomain']\n",
    "#             LexNet = LexicographerNet[LexicographerNet['BabelNet'] == BNet.iloc[0]]['LexNames']\n",
    "#             print(list(m.iloc[0]), lemma, text, BNet.iloc[0], WordNetDomain, LexNet.iloc[0])\n",
    "#             csv_writer.writerow([instance_id,\n",
    "#                                 m['sensekey1'].iloc[0], m['sensekey2'].iloc[0], \n",
    "#                                 lemma, text, \n",
    "#                                 BNet.iloc[0], WordNetDomain.iloc[0], LexNet.iloc[0]])\n",
    "        elem.clear()\n",
    "del context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
