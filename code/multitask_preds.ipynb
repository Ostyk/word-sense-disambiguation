{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as K\n",
    "\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm, tnrange\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import utils\n",
    "import parsers\n",
    "import models\n",
    "import generatorMultitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_path = '../resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path ='../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.data.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "batch: : 3it [00:09,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done writing to:\ttest.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict.main_predict_multitask(input_path = input_path, output_path='test.txt', resources_path=resources_path, prediction_type='wordnet_domains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_predict_multitask(input_path, output_path, resources_path, prediction_type, batch_size=64, PADDING_SIZE=30):\n",
    "    \"\"\"\n",
    "    :param input_path:\n",
    "    :param output_path:\n",
    "    :param resources_path:\n",
    "    :param prediction_type: either babelnet, wordnet_domains, or lexicographer\n",
    "    :param batch_size: depends on the model\n",
    "    :param PADDING_SIZE: depends on the model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    K.backend.clear_session()\n",
    "\n",
    "    # ##################\n",
    "    # # vocab loading #\n",
    "    # #################\n",
    "    mapping = pd.read_csv(os.path.join(resources_path, \"mapping.csv\"))\n",
    "\n",
    "    senses = utils.json_vocab_reader(os.path.join(resources_path, 'semcor.vocab.WordNet.json'))\n",
    "    wordnet_domains_vocabulary = utils.json_vocab_reader(os.path.join(resources_path, 'semcor.vocab.WordNetDomain.json'))\n",
    "    lexicographer_vocabulary = utils.json_vocab_reader(os.path.join(resources_path, 'semcor.vocab.LexNames.json'))\n",
    "\n",
    "    inputs, antivocab = utils.json_vocab_reader(os.path.join(resources_path, 'semcor.input.vocab.json'),\n",
    "                                                os.path.join(resources_path, 'semcor.leftout.vocab.json'))\n",
    "\n",
    "    output_vocab = utils.merge_vocabulary(senses, inputs)\n",
    "    output_vocab2 = utils.merge_vocabulary(wordnet_domains_vocabulary, inputs)\n",
    "    output_vocab3 = utils.merge_vocabulary(lexicographer_vocabulary, inputs)\n",
    "\n",
    "    reverse_output1_vocab =  dict((v, k) for k, v in output_vocab.items())\n",
    "    reverse_output2_vocab =  dict((v, k) for k, v in output_vocab2.items())\n",
    "    reverse_output3_vocab =  dict((v, k) for k, v in output_vocab3.items())\n",
    "    \n",
    "    # ##################\n",
    "    # # Model loading #\n",
    "    # #################\n",
    "    model_path, model_weight_path = sorted([os.path.join(resources_path,\n",
    "                                        os.path.join('models/best_model', i)) for i in os.listdir(\n",
    "                                        os.path.join(resources_path, 'models/best_model')) if i.startswith(\"model\")])\n",
    "\n",
    "    model_path = os.path.join(resources_path, 'models/model_2019-09-12_14:24:25_+0200.h5')\n",
    "    model_weight_path = os.path.join(resources_path, 'models/model_weights_2019-09-12_14:24:25_+0200.h5')\n",
    "\n",
    "    loaded_model = K.models.load_model(model_path)\n",
    "    loaded_model.load_weights(model_weight_path)\n",
    "    \n",
    "    ### generator\n",
    "    eval_generator = generatorMultitask.get(batch_size = 64,\n",
    "                                        resources_path = resources_path,\n",
    "                                        training_file_path = input_path,\n",
    "                                        antivocab = antivocab,\n",
    "                                        output_vocab = output_vocab,\n",
    "                                        output_vocab2 = output_vocab2,\n",
    "                                        output_vocab3 = output_vocab3,\n",
    "                                        PADDING_SIZE = PADDING_SIZE)\n",
    "\n",
    "    real_words = eval_parser(path = input_path, batch_size = batch_size)\n",
    "    \n",
    "    \n",
    "    with open(output_path, mode=\"a\") as out:\n",
    "        for batch_ground_truth_sentences in tqdm(real_words, desc='batch: '):\n",
    "\n",
    "            batch_x, candidate_synsets_wordnet, candidates_wndomain, candidates_lex = next(eval_generator)\n",
    "\n",
    "            batch_model_predictions = loaded_model.predict_on_batch(batch_x)\n",
    "\n",
    "            if prediction_type =='babelnet': #actually using babelnet\n",
    "                batch_model_predictions = batch_model_predictions[0]\n",
    "                reverse_output_vocab = reverse_output1_vocab \n",
    "                candidate_synsets = candidate_synsets_wordnet\n",
    "\n",
    "            elif prediction_type =='wordnet_domains': \n",
    "                batch_model_predictions = batch_model_predictions[1]\n",
    "                reverse_output_vocab = reverse_output2_vocab\n",
    "                candidate_synsets = candidates_wndomain\n",
    "\n",
    "            elif prediction_type =='lexicographer': \n",
    "                batch_model_predictions = batch_model_predictions[2] \n",
    "                reverse_output_vocab = reverse_output3_vocab\n",
    "                candidate_synsets = candidates_lex\n",
    "\n",
    "\n",
    "            batch_outputs = basic_predict(batch_ground_truth_sentences,\n",
    "                                          batch_model_predictions,\n",
    "                                          candidate_synsets,\n",
    "                                          PADDING_SIZE,\n",
    "                                          reverse_output_vocab)\n",
    "            for line in batch_outputs:\n",
    "                if not line.success:\n",
    "                    pred = mapping[mapping.WordNet==line.WordNet][prediction_type].values\n",
    "                    assert len(pred)==1, \"error in mapping {}\" .format(line.WordNet)\n",
    "                    fmt = \"{} {} \\n\".format(line.Sentence_id, pred[0])\n",
    "                else:\n",
    "                    fmt = \"{} {} \\n\".format(line.Sentence_id, line.WordNet) #not really wordnet in this case\n",
    "            out.write(fmt)\n",
    "    print(\"done writing to:\\t{}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################\n",
    "# # Model loading #\n",
    "# #################\n",
    "model_path, model_weight_path = sorted([os.path.join(resources_path,\n",
    "                                    os.path.join('models/best_model', i)) for i in os.listdir(\n",
    "                                    os.path.join(resources_path, 'models/best_model')) if i.startswith(\"model\")])\n",
    "\n",
    "model_path = os.path.join(resources_path, 'models/model_2019-09-09_21:26:42_+0000.h5')\n",
    "model_weight_path = os.path.join(resources_path, 'models/model_weights_2019-09-12_14:24:25_+0200.h5')\n",
    "\n",
    "loaded_model = K.models.load_model(model_path)\n",
    "loaded_model.load_weights(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_generator = generatorMultitask.get(batch_size = 64,\n",
    "#                                         resources_path = resources_path,\n",
    "#                                         training_file_path = input_path,\n",
    "#                                         antivocab = antivocab,\n",
    "#                                         output_vocab = output_vocab,\n",
    "#                                         output_vocab2 = output_vocab2,\n",
    "#                                         output_vocab3 = output_vocab3,\n",
    "#                                         PADDING_SIZE = PADDING_SIZE)\n",
    "# batch_x, candidate_synsets_wordnet, candidates_wndomain, candidates_lex = next(eval_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_x, candidate_synsets, candidates_wndomain, candidates_lex = next(eval_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = generatorMultitask.get(batch_size = 64,\n",
    "                                        resources_path = resources_path,\n",
    "                                        training_file_path = input_path,\n",
    "                                        antivocab = antivocab,\n",
    "                                        output_vocab = output_vocab,\n",
    "                                        output_vocab2 = output_vocab2,\n",
    "                                        output_vocab3 = output_vocab3,\n",
    "                                        PADDING_SIZE = PADDING_SIZE)\n",
    "\n",
    "real_words = eval_parser(path = input_path, batch_size = batch_size)\n",
    "\n",
    "prediction_type = 'wordnet_domains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_ground_truth_sentences in tqdm(real_words, desc='batch: '):\n",
    "\n",
    "    batch_x, candidate_synsets_wordnet, candidates_wndomain, candidates_lex = next(eval_generator)\n",
    "\n",
    "    batch_model_predictions = loaded_model.predict_on_batch(batch_x)\n",
    "    \n",
    "    if prediction_type =='babelnet': #actually using babelnet\n",
    "        batch_model_predictions = batch_model_predictions[0]\n",
    "        reverse_output_vocab = reverse_output1_vocab \n",
    "        candidate_synsets = candidate_synsets_wordnet\n",
    "        \n",
    "    elif prediction_type =='wordnet_domains': \n",
    "        batch_model_predictions = batch_model_predictions[1]\n",
    "        reverse_output_vocab = reverse_output2_vocab\n",
    "        candidate_synsets = candidates_wndomain\n",
    "        \n",
    "    elif prediction_type =='lexicographer': \n",
    "        batch_model_predictions = batch_model_predictions[2] \n",
    "        reverse_output_vocab = reverse_output3_vocab\n",
    "        candidate_synsets = candidates_lex\n",
    "    \n",
    "    \n",
    "    batch_outputs = basic_predict(batch_ground_truth_sentences,\n",
    "                                  batch_model_predictions,\n",
    "                                  candidate_synsets,\n",
    "                                  PADDING_SIZE,\n",
    "                                  reverse_output_vocab)\n",
    "    break\n",
    "batch_model_predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in batch_outputs:\n",
    "    if not line.success:\n",
    "        pred = mapping[mapping.WordNet==line.WordNet][prediction_type].values\n",
    "        assert len(pred)==1, \"error in mapping {}\" .format(line.WordNet)\n",
    "        fmt = \"{} {} \\n\".format(line.Sentence_id, pred[0])\n",
    "    else:\n",
    "        fmt = \"{} {} \\n\".format(line.Sentence_id, line.WordNet) #not really wordnet in this case\n",
    "out.write(fmt)\n",
    "    print(\"done writing to:\\t{}\".format(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(prob_dist_candidate_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.MFS.retrieve_item(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_outputs = MultitaskPredict(batch_ground_truth_sentences, \n",
    "                                 batch_model_predictions, \n",
    "                                 candidate_synsets, candidates_wndomain, candidates_lex,\n",
    "                                 PADDING_SIZE,\n",
    "                                 reverse_output_vocab, reverse_output2_vocab, reverse_output3_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_outputs[0].success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_predict(batch_ground_truth_sentences,\n",
    "                  batch_model_predictions, \n",
    "                  candidate_synsets, \n",
    "                  PADDING_SIZE,\n",
    "                  reverse_output_vocab):\n",
    "    \"\"\"\n",
    "    Peforms predictions on a batch for the basic model\n",
    "    :param batch_ground_truth_sentences:\n",
    "    :param batch_model_predictions:\n",
    "    :param candidate_synsets candidate_synsets:\n",
    "    :param PADDING_SIZE:\n",
    "    :param reverse_output_vocab:\n",
    "    return predicted: batch of tuple(Sentence_id, WordNet)\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs = []\n",
    "    output = namedtuple(\"output\", \"Sentence_id WordNet success\")\n",
    "    \n",
    "    for idx_sentence, sentence in enumerate(batch_model_predictions):\n",
    "\n",
    "        ground_truth_sentence = batch_ground_truth_sentences[idx_sentence]\n",
    "        \n",
    "        for idx, entry in enumerate(ground_truth_sentence):\n",
    "    \n",
    "            if entry.instance == True: #only for instances not wf\n",
    "                if idx<PADDING_SIZE: \n",
    "                    #WSD argmax\n",
    "                    word_prob = sentence[idx]\n",
    "                    current_candidate_synsets = candidate_synsets[idx_sentence][idx]\n",
    "                    prob_dist_candidate_synset = word_prob[current_candidate_synsets]\n",
    "                    current_synset = np.argmax(prob_dist_candidate_synset)\n",
    "\n",
    "                    if current_synset>4: #change after deleting start stop\n",
    "                        item = output(Sentence_id = entry.id_, WordNet = reverse_output_vocab[current_synset], success = True)\n",
    "                        outputs.append(item)\n",
    "                    else: #fallback\n",
    "                        word = entry.lemma\n",
    "                        item = output(Sentence_id = entry.id_, WordNet = models.MFS.retrieve_item(word), success = False)\n",
    "                        outputs.append(item)\n",
    "                else: #predict truncated\n",
    "                    word = entry.lemma\n",
    "                    item = output(Sentence_id = entry.id_, WordNet = models.MFS.retrieve_item(word), success = False)\n",
    "                    outputs.append(item)\n",
    "                \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultitaskPredict(batch_ground_truth_sentences, \n",
    "                     batch_model_predictions, \n",
    "                     candidate_synsets, candidates_wndomain, candidates_lex,\n",
    "                     PADDING_SIZE,\n",
    "                     reverse_output_vocab, reverse_output2_vocab, reverse_output3_vocab):\n",
    "    \"\"\"\n",
    "    Peforms predictions on a batch for the MultiTask\n",
    "    :param batch_ground_truth_sentences:\n",
    "    :param batch_model_predictions:\n",
    "    :param candidate_synsets candidate_synsets:\n",
    "    :param PADDING_SIZE:\n",
    "    :param reverse_output_vocab:\n",
    "    return predicted: batch of tuple(Sentence_id, WordNet)\n",
    "    \"\"\"\n",
    "    batch_model_predictions_wndomain = batch_model_predictions[1]\n",
    "    batch_model_predictions_wndomain = batch_model_predictions[2]\n",
    "    batch_model_predictions = batch_model_predictions[0]\n",
    "    \n",
    "    outputs = []\n",
    "    output = namedtuple(\"output\", \"Sentence_id WordNet\")\n",
    "    \n",
    "    for idx_sentence, sentence in enumerate(batch_model_predictions):\n",
    "\n",
    "        ground_truth_sentence = batch_ground_truth_sentences[idx_sentence]\n",
    "        \n",
    "        for idx, entry in enumerate(ground_truth_sentence):\n",
    "    \n",
    "            if entry.instance == True: #only for instances not wf\n",
    "                if idx<PADDING_SIZE: \n",
    "                    #WSD argmax\n",
    "                    word_prob = sentence[idx]\n",
    "                    current_candidate_synsets = candidate_synsets[idx_sentence][idx]\n",
    "                    prob_dist_candidate_synset = word_prob[current_candidate_synsets]\n",
    "                    current_synset = np.argmax(prob_dist_candidate_synset)\n",
    "\n",
    "                    if current_synset>4: #change after deleting start stop\n",
    "                        item = output(Sentence_id = entry.id_, WordNet = reverse_output_vocab[current_synset])\n",
    "                        outputs.append(item)\n",
    "                    else: #fallback\n",
    "                        word = entry.lemma\n",
    "                        item = output(Sentence_id = entry.id_, WordNet = models.MFS.retrieve_item(word))\n",
    "                        outputs.append(item)\n",
    "                else: #predict truncated\n",
    "                    word = entry.lemma\n",
    "                    item = output(Sentence_id = entry.id_, WordNet = models.MFS.retrieve_item(word))\n",
    "                    outputs.append(item)\n",
    "                \n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
