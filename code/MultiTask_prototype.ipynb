{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, tnrange\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import utils\n",
    "import parsers\n",
    "from nltk.corpus import wordnet as wn\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.xml'\n",
    "gold_file_path =  '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.txt'\n",
    "\n",
    "training_file_path = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2015/semeval2015.data.xml'\n",
    "gold_file_path =  '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt'\n",
    "\n",
    "training_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n",
    "gold_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n",
    "\n",
    "fine_senses_vocab_path = '../resources/semcor.vocab.WordNet.json'\n",
    "wordnet_domains_vocab_path = '../resources/semcor.vocab.WordNetDomain.json'\n",
    "lexicographer_vocab_path = '../resources/semcor.vocab.LexNames.json'\n",
    "\n",
    "input_vocab_path = '../resources/semcor.input.vocab.json'\n",
    "input_antivocab_path = '../resources/semcor.leftout.vocab.json'\n",
    "embedding_size = 64\n",
    "batch_size = 64\n",
    "LEARNING_RATE = 0.1\n",
    "N_EPOCHS = 10\n",
    "PADDING_SIZE = 30\n",
    "print_model = False\n",
    "\n",
    "\n",
    "#loading dict\n",
    "senses = utils.json_vocab_reader(fine_senses_vocab_path)\n",
    "wordnet_domains_vocabulary = utils.json_vocab_reader(wordnet_domains_vocab_path)\n",
    "lexicographer_vocabulary = utils.json_vocab_reader(lexicographer_vocab_path)\n",
    "\n",
    "inputs, antivocab = utils.json_vocab_reader(input_vocab_path, input_antivocab_path)\n",
    "output_vocab = utils.merge_vocabulary(senses, inputs)\n",
    "output_vocab2 = utils.merge_vocabulary(wordnet_domains_vocabulary, inputs)\n",
    "output_vocab3 = utils.merge_vocabulary(lexicographer_vocabulary, inputs)\n",
    "\n",
    "\n",
    "\n",
    "reverse_output_vocab =  dict((v, k) for k, v in output_vocab.items())\n",
    "\n",
    "K.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generatorMultitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generatorMultitask.get(batch_size = 64,\n",
    "                                resources_path = '../resources',\n",
    "                                training_file_path = training_file_path,\n",
    "                                gold_file_path = gold_file_path,\n",
    "                                antivocab = antivocab,\n",
    "                                output_vocab = output_vocab,\n",
    "                                output_vocab2 = output_vocab2,\n",
    "                                output_vocab3 = output_vocab3,\n",
    "                                PADDING_SIZE = PADDING_SIZE)\n",
    "\n",
    "validation_generator = generatorMultitask.get(batch_size = 64,\n",
    "                                resources_path = '../resources',\n",
    "                                training_file_path = training_file_path_dev,\n",
    "                                gold_file_path = gold_file_path_dev,\n",
    "                                antivocab = antivocab,\n",
    "                                output_vocab = output_vocab,\n",
    "                                output_vocab2 = output_vocab2,\n",
    "                                output_vocab3 = output_vocab3,\n",
    "                                PADDING_SIZE = PADDING_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multitask(vocab_size, embedding_size, hidden_size, PADDING_SIZE, LEARNING_RATE, INPUT_DROPOUT, LSTM_DROPOUT,RECURRENT_DROPOUT, N_EPOCHS):\n",
    "    print(\"Creating MultitaskKERAS model\")\n",
    "    inputs = K.layers.Input(shape=(PADDING_SIZE,))\n",
    "    embeddings = K.layers.Embedding(vocab_size['senses'],\n",
    "                                    embedding_size,\n",
    "                                    mask_zero=True,\n",
    "                                    name = 'embedding')(inputs)\n",
    "\n",
    "\n",
    "    BI_LSTM = (K.layers.Bidirectional(\n",
    "               K.layers.LSTM(hidden_size, dropout = LSTM_DROPOUT,\n",
    "                             recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                             return_sequences=True,\n",
    "                             kernel_regularizer=K.regularizers.l2(0.01),\n",
    "                             activity_regularizer=K.regularizers.l1(0.01)\n",
    "                            ), name = 'Bi-directional_LSTM'))(embeddings)\n",
    "\n",
    "    predictions_1 = K.layers.TimeDistributed(K.layers.Dense(\n",
    "        vocab_size['senses'], activation='softmax', name='senses'))(BI_LSTM)\n",
    "    \n",
    "    predictions_2 = K.layers.TimeDistributed(K.layers.Dense(\n",
    "        vocab_size['wordnet_domains'], activation='softmax', name = 'wordnet_domains'))(BI_LSTM)\n",
    "    \n",
    "    predictions_3 = K.layers.TimeDistributed(K.layers.Dense(\n",
    "        vocab_size['lexicographer'],activation='softmax', name = 'lexicographer'))(BI_LSTM)\n",
    "\n",
    "\n",
    "    model = K.models.Model(inputs=[inputs], outputs=[predictions_1,\n",
    "                                                     predictions_2,\n",
    "                                                     predictions_3])\n",
    "\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\",#,\"sparse_categorical_crossentropy\",\"sparse_categorical_crossentropy\"],\n",
    "                  optimizer = K.optimizers.Adam(lr=LEARNING_RATE, decay = 0.001/N_EPOCHS, amsgrad=False),\n",
    "                  metrics = ['acc'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = {\"senses\": len(output_vocab),\n",
    "               \"wordnet_domains\": len(output_vocab2),\n",
    "               \"lexicographer\": len(output_vocab3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MultitaskKERAS model\n"
     ]
    }
   ],
   "source": [
    "MultiTaskModelNetwork = Multitask(vocab_size = vocab_sizes,\n",
    "                                embedding_size = embedding_size,\n",
    "                                hidden_size = 32,\n",
    "                                PADDING_SIZE = PADDING_SIZE,\n",
    "                                LEARNING_RATE = LEARNING_RATE,\n",
    "                                INPUT_DROPOUT = 0.25,\n",
    "                                LSTM_DROPOUT = 0.45,\n",
    "                                RECURRENT_DROPOUT = 0.30,\n",
    "                                N_EPOCHS = N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MultiTaskModelNetwork = models.Multitask(vocab_size = vocab_sizes,\n",
    "                                embedding_size = embedding_size,\n",
    "                                hidden_size = 32,\n",
    "                                PADDING_SIZE = PADDING_SIZE,\n",
    "                                LEARNING_RATE = LEARNING_RATE,\n",
    "                                INPUT_DROPOUT = 0.25,\n",
    "                                LSTM_DROPOUT = 0.45,\n",
    "                                RECURRENT_DROPOUT = 0.30,\n",
    "                                N_EPOCHS = N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n"
     ]
    }
   ],
   "source": [
    "BasicModelNetwork = models.Basic(vocab_size = len(output_vocab),\n",
    "                                embedding_size = embedding_size,\n",
    "                                hidden_size = 32,\n",
    "                                PADDING_SIZE = PADDING_SIZE,\n",
    "                                LEARNING_RATE = LEARNING_RATE,\n",
    "                                INPUT_DROPOUT = 0.25,\n",
    "                                LSTM_DROPOUT = 0.45,\n",
    "                                RECURRENT_DROPOUT = 0.30,\n",
    "                                N_EPOCHS = N_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.utils.plot_model(BasicModelNetwork, to_file='../report/img/basic.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if not os.path.exists('../resources/logging'):\n",
    "    os.mkdir('../resources/logging')\n",
    "model_name = time.strftime('%Y-%m-%d_%H:%M:%S_%z')\n",
    "cbk = K.callbacks.TensorBoard('../resources/logging/keras_model_'+model_name)\n",
    "\n",
    "early_stopping = K.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=1,\n",
    "                              verbose=1, mode='auto')\n",
    "csv_logger = K.callbacks.CSVLogger('../resources/logging/keras_model_'+model_name+'.log')\n",
    "model_checkpoint = K.callbacks.ModelCheckpoint(filepath = '../resources/logging/keras_model_'+model_name+'.h5',\n",
    "                                               monitor='val_precision',\n",
    "                                               verbose=2,\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True,\n",
    "                                               mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = generatorMultitask.__len__(training_file_path, batch_size)\n",
    "val_len = generatorMultitask.__len__(training_file_path_dev, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - 68s 34s/step - loss: 31.3608 - time_distributed_loss: 10.6404 - time_distributed_1_loss: 9.8269 - time_distributed_2_loss: 9.8181 - time_distributed_acc: 0.0076 - time_distributed_1_acc: 0.0868 - time_distributed_2_acc: 0.0852 - val_loss: 22.5992 - val_time_distributed_loss: 8.2868 - val_time_distributed_1_loss: 6.9529 - val_time_distributed_2_loss: 6.8783 - val_time_distributed_acc: 0.2669 - val_time_distributed_1_acc: 0.2499 - val_time_distributed_2_acc: 0.2499\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 51s 25s/step - loss: 14.2050 - time_distributed_loss: 5.2651 - time_distributed_1_loss: 3.9290 - time_distributed_2_loss: 4.1355 - time_distributed_acc: 0.4046 - time_distributed_1_acc: 0.3974 - time_distributed_2_acc: 0.3974 - val_loss: 18.9733 - val_time_distributed_loss: 6.4162 - val_time_distributed_1_loss: 5.3543 - val_time_distributed_2_loss: 5.5258 - val_time_distributed_acc: 0.2823 - val_time_distributed_1_acc: 0.2850 - val_time_distributed_2_acc: 0.2794\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 53s 26s/step - loss: 11.5905 - time_distributed_loss: 3.9923 - time_distributed_1_loss: 2.6829 - time_distributed_2_loss: 3.1813 - time_distributed_acc: 0.4039 - time_distributed_1_acc: 0.3886 - time_distributed_2_acc: 0.4085 - val_loss: 18.6018 - val_time_distributed_loss: 6.1715 - val_time_distributed_1_loss: 5.5951 - val_time_distributed_2_loss: 5.1029 - val_time_distributed_acc: 0.5662 - val_time_distributed_1_acc: 0.3436 - val_time_distributed_2_acc: 0.5662\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 63s 32s/step - loss: 9.5922 - time_distributed_loss: 3.1971 - time_distributed_1_loss: 2.2153 - time_distributed_2_loss: 2.5206 - time_distributed_acc: 0.5581 - time_distributed_1_acc: 0.5050 - time_distributed_2_acc: 0.5468 - val_loss: 18.2979 - val_time_distributed_loss: 6.4606 - val_time_distributed_1_loss: 5.2540 - val_time_distributed_2_loss: 5.1500 - val_time_distributed_acc: 0.2825 - val_time_distributed_1_acc: 0.2866 - val_time_distributed_2_acc: 0.2865\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 59s 30s/step - loss: 9.0440 - time_distributed_loss: 3.0250 - time_distributed_1_loss: 2.1383 - time_distributed_2_loss: 2.5373 - time_distributed_acc: 0.4034 - time_distributed_1_acc: 0.4176 - time_distributed_2_acc: 0.4082 - val_loss: 16.6116 - val_time_distributed_loss: 6.0925 - val_time_distributed_1_loss: 4.6951 - val_time_distributed_2_loss: 4.6340 - val_time_distributed_acc: 0.5040 - val_time_distributed_1_acc: 0.5382 - val_time_distributed_2_acc: 0.5303\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0bf06d750cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicModelNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "MultiTaskModelNetwork.fit_generator(train_generator, \n",
    "                                steps_per_epoch=train_len,\n",
    "                                epochs=5, \n",
    "                                verbose=1,\n",
    "                                callbacks=[cbk, early_stopping],\n",
    "                                validation_data=validation_generator,\n",
    "                                validation_steps=val_len,\n",
    "                                class_weight=None,\n",
    "                                max_queue_size=10,\n",
    "                                workers=-1, \n",
    "                                use_multiprocessing=True,\n",
    "                                shuffle=False,\n",
    "                                initial_epoch=0)\n",
    "\n",
    "\n",
    "models.save_model(model = BasicModelNetwork, model_name = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.833333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "580*30/60/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
