{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import utils\n",
    "import keras as K\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.xml'\n",
    "gold_file_path =  '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.txt'\n",
    "training_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n",
    "gold_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n",
    "fine_senses_vocab_path = '../resources/semcor.vocab.WordNet.json'\n",
    "input_vocab_path = '../resources/semcor.input.vocab.json'\n",
    "input_antivocab_path = '../resources/semcor.leftout.vocab.json'\n",
    "embedding_size = 32\n",
    "batch_size = 64\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 10\n",
    "PADDING_SIZE = 50\n",
    "print_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_file_path = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n",
    "# gold_file_path = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dict\n",
    "senses = utils.json_vocab_reader(fine_senses_vocab_path)\n",
    "inputs, antivocab = utils.json_vocab_reader(input_vocab_path, input_antivocab_path)\n",
    "output_vocab = utils.vocab_merge(senses, inputs)\n",
    "reverse_output_vocab =  dict((v, k) for k, v in output_vocab.items())\n",
    "\n",
    "K.backend.clear_session()\n",
    "model = models.Basic(test=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_generator = model.prepare_sentence_batch(batch_size = 64,\n",
    "#                                                      training_file_path = training_file_path_dev,\n",
    "#                                                      gold_file_path = gold_file_path_dev,\n",
    "#                                                      antivocab = antivocab,\n",
    "#                                                      output_vocab = output_vocab,\n",
    "#                                                      PADDING_SIZE = PADDING_SIZE)\n",
    "\n",
    "\n",
    "# for i, j in validation_generator:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_flow = parsers.TrainingParser(training_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37176it [00:05, 6721.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37176, 37176)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = 0\n",
    "for batch_count, sentence in enumerate(tqdm(training_data_flow.parse()), start = 1):\n",
    "    sentences +=1\n",
    "sentences,batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\"sentence\" : [], \"labels\" : [], \"candidates\": []}\n",
    "for entry in sentence:\n",
    "\n",
    "            id_, lemma, pos, _ = entry\n",
    "            \n",
    "            output_word = utils.replacement_routine(lemma, entry, antivocab, output_vocab)        \n",
    "            output['sentence'].append(output_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'candidates': [], 'labels': [], 'sentence': [4, 0, 0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import utils\n",
    "import parsers\n",
    "from nltk.corpus import wordnet as wn\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n",
    "gold_file_path = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic(object):\n",
    "    \"\"\"\n",
    "    Word Sense Disambiguiation performed via a basic sequence tagging\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, training_file_path, antivocab, output_vocab, PADDING_SIZE = 50, gold_file_path = None):\n",
    "\n",
    "        self.batch_size =  batch_size\n",
    "        self.training_file_path =  training_file_path\n",
    "        self.antivocab =  antivocab\n",
    "        self.output_vocab =  output_vocab\n",
    "        self.PADDING_SIZE =  PADDING_SIZE\n",
    "        self.gold_file_path  =  gold_file_path\n",
    "        self.length = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return parsers.TrainingParser(self.training_file_path).count()\n",
    "\n",
    "    def __getitem__(self):\n",
    "        \"\"\"\n",
    "        Batch procesing generator, yields a dict of sentences, candidates and labels if in training mode (determined if gold_file_path is specified)\n",
    "\n",
    "        param batch_size:\n",
    "        param training_file_path:\n",
    "        param antivocab:\n",
    "        param output_vocab:\n",
    "        param gold_file_path:\n",
    "        return: generator object\n",
    "        \"\"\"\n",
    "        batch = {\"sentences\" : [], \"candidates\" : []}\n",
    "\n",
    "        training_data_flow = parsers.TrainingParser(self.training_file_path )\n",
    "        if self.gold_file_path:\n",
    "            self.gold_data_flow = parsers.GoldParser(self.gold_file_path)\n",
    "            batch.update({\"labels\" : []})\n",
    "\n",
    "\n",
    "        for batch_count, sentence in enumerate(training_data_flow.parse(), start = 1):\n",
    "            self.length += 1\n",
    "            #training mode\n",
    "            if self.gold_file_path:\n",
    "                labels = self.gold_data_flow.parse()\n",
    "                output = self.prepare_sentence(sentence, self.antivocab, self.output_vocab, labels)\n",
    "\n",
    "                batch['sentences'].append(output['sentence'])\n",
    "                batch['candidates'].append(output['candidates'])\n",
    "                batch['labels'].append(output['labels'])\n",
    "\n",
    "            #evaulation mode\n",
    "            else:\n",
    "                output = self.prepare_sentence(sentence, antivocab, output_vocab)\n",
    "\n",
    "                batch['sentences'].append(output['sentence'])\n",
    "                batch['candidates'].append(output['candidates'])\n",
    "\n",
    "            if int(batch_count)%int(self.batch_size)==0:\n",
    "\n",
    "                for key in batch.keys():\n",
    "                    batch[key] = self.apply_padding(batch, key, maxlen = self.PADDING_SIZE, value = 1)\n",
    "                \n",
    "                \n",
    "                batch_count = 0\n",
    "                \n",
    "                if self.gold_file_path:\n",
    "                    yield batch['sentences'], np.expand_dims(batch['labels'], axis=-1)\n",
    "                else:\n",
    "                    yield batch['sentences']\n",
    "                batch = {\"sentences\" : [], \"candidates\" : []}\n",
    "                if self.gold_file_path:\n",
    "                    batch.update({\"labels\" : []})\n",
    "                    \n",
    "        if batch_count>0:\n",
    "            print(batch_count)\n",
    "            for key in batch.keys():\n",
    "                    batch[key] = self.apply_padding(batch, key, maxlen = self.PADDING_SIZE, value = 1)\n",
    "            batch_count = 0\n",
    "            \n",
    "            if self.gold_file_path:\n",
    "                x, y = batch['sentences'], np.expand_dims(batch['labels'], axis=-1)\n",
    "                yield shuffle(x, y)\n",
    "            else:\n",
    "                yield shuffle(batch['sentences'])\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_padding(output, key, maxlen=50, value=1):\n",
    "        \"\"\"\n",
    "        Applies padding to output sequences\n",
    "\n",
    "        param output: dict\n",
    "        param key: key of dict\n",
    "        param maxlen: length to pad\n",
    "        param value: pad with this value\n",
    "        return padded list of lists\n",
    "        \"\"\"\n",
    "        x = output[key]\n",
    "        if key == 'candidates':\n",
    "            for candidate in range(len(x)):\n",
    "                x[candidate] =  x[candidate] + [[value]] * (maxlen-len(x[candidate]))\n",
    "            return x\n",
    "        else:\n",
    "            return K.preprocessing.sequence.pad_sequences(x, truncating='pre', padding='post', maxlen=maxlen, value = value )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_sentence(sentence, antivocab, output_vocab, labels=None):\n",
    "        \"\"\"\n",
    "        Prepares an output sentence consisting of the sentence itself along with labels and candidates\n",
    "\n",
    "        param sentence:\n",
    "        param antivocab:\n",
    "        param output_vocab:\n",
    "        param labels:\n",
    "\n",
    "        return output: dict with keys: sentence, labels, candidates all list type objects\n",
    "        \"\"\"\n",
    "        records = namedtuple(\"Training\", \"id_ lemma pos instance\")\n",
    "\n",
    "        output = {\"sentence\" : [], \"labels\" : [], \"candidates\": []}\n",
    "        for entry in sentence:\n",
    "\n",
    "            id_, lemma, pos, _ = entry\n",
    "\n",
    "            output_word = utils.replacement_routine(lemma, entry, antivocab, output_vocab)\n",
    "            output['sentence'].append(output_word)\n",
    "\n",
    "            if id_ is None:\n",
    "                output['labels'].append(output_word)\n",
    "                candidates = [output_word]\n",
    "\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    current_label = labels.__next__()\n",
    "                    assert current_label.id_ == id_, \"ID mismatch\"\n",
    "\n",
    "                    sense = current_label.senses[0]\n",
    "                    sense = output_vocab[sense] if sense in output_vocab else output_vocab[\"<UNK>\"]\n",
    "                    output['labels'].append(sense)\n",
    "                candidates = utils.candidate_synsets(lemma, pos)\n",
    "                candidates = [utils.replacement_routine(c, records(id_=None, lemma=c, pos=\"X\", instance=True), antivocab, output_vocab) for c in candidates]\n",
    "\n",
    "            output['candidates'].append(candidates)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Basic(batch_size,\n",
    "      training_file_path,\n",
    "      antivocab,\n",
    "      output_vocab,\n",
    "      50,\n",
    "      gold_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37176"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n",
      "(64, 50)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-0e8029ede7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-26b58607d109>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgold_file_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgold_data_flow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantivocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-26b58607d109>\u001b[0m in \u001b[0;36mprepare_sentence\u001b[0;34m(sentence, antivocab, output_vocab, labels)\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplacement_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantivocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candidates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-26b58607d109>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplacement_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantivocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candidates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/word-sense-disambiguation/code/utils.py\u001b[0m in \u001b[0;36mreplacement_routine\u001b[0;34m(element, entry, antivocab, output_vocab)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplacement_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantivocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mret_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mantivocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mret_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<REPLACEMENT>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, j  in x.__getitem__():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reverse_output_vocab[q] for q in i[-2,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_flow = parsers.TrainingParser(training_file_path )\n",
    "\n",
    "\n",
    "for batch_count, sentence in enumerate(training_data_flow.parse(), start = 1):\n",
    "    if int(batch_count)%int(batch_size)==0:\n",
    "        print(batch_count)\n",
    "    if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(i, np.squeeze(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(object):\n",
    "    def __init__(self, t):\n",
    "        self.t = t\n",
    "        self.length = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def sex(self):\n",
    "        self.length = 2\n",
    "        return self.t**2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
