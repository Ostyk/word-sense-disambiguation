{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from collections import Counter, namedtuple\n",
    "import multiprocessing\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "import pickle\n",
    "import keras as K\n",
    "import tensorflow\n",
    "from nltk.corpus import wordnet as wn\n",
    "import xml.etree.ElementTree as ET\n",
    "import keras\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel(vocab_size,vocab_size2,vocab_size3,embedding_size, hidden_size, epochs):\n",
    "    print(\"Model is starting\")\n",
    "    model=K.models.Sequential()\n",
    "    \n",
    "    #Embedding layer\n",
    "    #model.add(K.layers.Embedding(vocab_size,embedding_size,mask_zero=True,embeddings_initializer='uniform'))\n",
    "    \n",
    "    #Input layer, optional\n",
    "    x=K.layers.Input(shape=(50,))\n",
    "    X_x=K.layers.Embedding(vocab_size,32,mask_zero=True)(x)\n",
    "    #LSTM layer\n",
    "    lstm=(K.layers.Bidirectional(K.layers.LSTM(hidden_size,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))(X_x)\n",
    "    #lstm=(K.layers.Bidirectional(K.layers.LSTM(hidden_size,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))(lstmforw)\n",
    "              \n",
    "    #MERGER\n",
    "\n",
    "    #attention goes here\n",
    "    #possible other lstm layer\n",
    "              \n",
    "    #dense layer\n",
    "\n",
    "    y1=K.layers.TimeDistributed(K.layers.Dense(vocab_size,activation=\"softmax\",name=\"senses\"))(lstm)\n",
    "    y2=K.layers.TimeDistributed(K.layers.Dense(vocab_size2,activation=\"softmax\",name=\"domains\"))(lstm)          \n",
    "    y3=K.layers.TimeDistributed(K.layers.Dense(vocab_size3,activation=\"softmax\",name=\"lex\"))(lstm)          \n",
    "    #y1=K.layers.Flatten()(y1)\n",
    "    #create model\n",
    "    model=K.models.Model(inputs=[x],outputs=[y1,y2,y3])\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer=K.optimizers.Adam(lr=0.001,decay=0.001/epochs,amsgrad=False)\n",
    "              \n",
    "    #Compile\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################\n",
    "### loading data ###\n",
    "####################\n",
    "SUBSET = ['pku','msr']\n",
    "PADDING_SIZE = 30\n",
    "data, uni_word_to_idx, bi_word_to_idx = data_feed(SUBSET, PADDING_SIZE)\n",
    "\n",
    "# saving critical info to be later used when used on test sets\n",
    "predict_dict = dict()\n",
    "predict_dict.update({\"PADDING_SIZE\": PADDING_SIZE})\n",
    "predict_dict.update({\"UNIGRAM_DICT\": uni_word_to_idx})\n",
    "predict_dict.update({\"BIGRAM_DICT\": bi_word_to_idx})\n",
    "\n",
    "rel_path = '../resources/vocabs/'\n",
    "if not os.path.exists(rel_path):\n",
    "    os.mkdir(rel_path)\n",
    "with open(rel_path + '_'.join(SUBSET) + \"_\" + str(PADDING_SIZE) + '.json', 'w') as f:\n",
    "    json.dump(predict_dict, f)\n",
    "\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "VOCAB_SIZE = {\"unigrams\": data['train']['info']['uni_VocabSize'],\n",
    "              \"bigrams\": data['train']['info']['bi_VocabSize']}\n",
    "EMBEDDING_SIZE = {\"unigrams\": 64,\n",
    "                  \"bigrams\": 16}\n",
    "HIDDEN_SIZE = 256\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "LEARNING_RATE = 0.0015\n",
    "INPUT_DROPOUT = 0.2\n",
    "LSTM_DROPOUT = 0.45\n",
    "RECURRENT_DROPOUT = 0.35\n",
    "\n",
    "#######################\n",
    "### Bulding model ###\n",
    "#######################\n",
    "K.backend.clear_session()\n",
    "\n",
    "model_name = time.strftime('%Y-%m-%d_%H:%M:%S_%z')\n",
    "def Model_A(vocab_size, embedding_size, hidden_size, PADDING_SIZE, LEARNING_RATE, INPUT_DROPOUT, LSTM_DROPOUT, RECURRENT_DROPOUT):\n",
    "    print(\"Creating KERAS model\")\n",
    "\n",
    "    unigrams = K.layers.Input(shape=(None,))\n",
    "    embedding_unigrams = K.layers.Embedding(vocab_size[\"unigrams\"],\n",
    "                                            embedding_size['unigrams'],\n",
    "                                            mask_zero=True,\n",
    "                                            name = 'embedding_unigrams')(unigrams)\n",
    "\n",
    "    bigrams = K.layers.Input(shape=(None,))\n",
    "    embedding_bigrams = K.layers.Embedding(vocab_size[\"bigrams\"],\n",
    "                                           embedding_size['bigrams'],\n",
    "                                           mask_zero=True,\n",
    "                                           name = 'embedding_bigrams')(bigrams)\n",
    "\n",
    "    merged_vector = K.layers.concatenate([embedding_unigrams, embedding_bigrams], axis=-1, name = 'concatenated')\n",
    "\n",
    "    BI_LSTM = (K.layers.Bidirectional(\n",
    "               K.layers.LSTM(hidden_size, dropout = LSTM_DROPOUT,\n",
    "                             recurrent_dropout = RECURRENT_DROPOUT,\n",
    "                             return_sequences=True,\n",
    "                             kernel_regularizer=K.regularizers.l2(0.01),\n",
    "                             activity_regularizer=K.regularizers.l1(0.01)\n",
    "                            ),name = 'Bi-directional_LSTM'))(merged_vector)\n",
    "\n",
    "    predictions = K.layers.TimeDistributed(K.layers.Dense(4, activation='softmax'))(BI_LSTM)\n",
    "\n",
    "    model = K.models.Model(inputs=[unigrams, bigrams], outputs=predictions)\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = K.optimizers.Adam(lr=LEARNING_RATE, clipnorm=1., clipvalue=0.5),\n",
    "                  metrics = ['acc', K.metrics.Precision()])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = Model_A(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE,\n",
    "                PADDING_SIZE, LEARNING_RATE, INPUT_DROPOUT,\n",
    "                LSTM_DROPOUT, RECURRENT_DROPOUT)\n",
    "# Let's print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "####################\n",
    "### loggin model ###\n",
    "####################\n",
    "if not os.path.exists('../resources/report_images'):\n",
    "    os.mkdir('../resources/report_images')\n",
    "\n",
    "cbk = K.callbacks.TensorBoard('../resources/logging/keras_model_'+model_name)\n",
    "print(\"\\nStarting training...\")\n",
    "K.utils.plot_model(model, to_file='../resources/report_images/model.png')\n",
    "\n",
    "early_stopping = K.callbacks.EarlyStopping(monitor='val_precision',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=2, mode='auto')\n",
    "csv_logger = K.callbacks.CSVLogger('../resources/logging/keras_model_'+model_name+'.log')\n",
    "model_checkpoint = K.callbacks.ModelCheckpoint(filepath = '../resources/logging/keras_model_'+model_name+'.h5',\n",
    "                                               monitor='val_precision',\n",
    "                                               verbose=2,\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True,\n",
    "                                               mode='auto', period=1)\n",
    "####################\n",
    "### training     ###\n",
    "####################\n",
    "\n",
    "model.fit(data[\"train\"][\"X\"], data[\"train\"][\"y\"],\n",
    "          epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "          validation_data=(data[\"dev\"][\"X\"], data[\"dev\"][\"y\"]),\n",
    "          callbacks=[cbk, csv_logger, model_checkpoint, early_stopping])\n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "rel_path = '../resources/models'\n",
    "if not os.path.exists(rel_path):\n",
    "    os.mkdir(rel_path)\n",
    "weights = os.path.join(rel_path,'model_weights_'+model_name+'_'.join(SUBSET)+'.h5')\n",
    "model_name_save = os.path.join(rel_path,'model_'+model_name+'_'.join(SUBSET)+'.h5')\n",
    "model.save_weights(weights) #saving weights for further analysis\n",
    "model.save(model_name_save)\n",
    "\n",
    "####################\n",
    "### Plotting     ###\n",
    "####################\n",
    "\n",
    "\n",
    "plot_training(model_name, True, PADDING_SIZE, epochs, '-'.join(SUBSET), size = 20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
