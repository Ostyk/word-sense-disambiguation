{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.xml'\n",
    "gold_file_path =  '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.txt'\n",
    "\n",
    "training_file_path = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n",
    "gold_file_path = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'\n",
    "\n",
    "training_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n",
    "gold_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt'\n",
    "fine_senses_vocab_path = '../resources/semcor.vocab.WordNet.json'\n",
    "input_vocab_path = '../resources/semcor.input.vocab.json'\n",
    "input_antivocab_path = '../resources/semcor.leftout.vocab.json'\n",
    "embedding_size = 32\n",
    "batch_size = 64\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 10\n",
    "PADDING_SIZE = 50\n",
    "print_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import utils\n",
    "import generators\n",
    "\n",
    "#from tensorflow.random import set_random_seed\n",
    "#set_random_seed(42)\n",
    "import tensorflow.keras as K\n",
    "#import keras as K\n",
    "import time\n",
    "import os\n",
    "#from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dict\n",
    "senses = utils.json_vocab_reader(fine_senses_vocab_path)\n",
    "inputs, antivocab = utils.json_vocab_reader(input_vocab_path, input_antivocab_path)\n",
    "output_vocab = utils.vocab_merge(senses, inputs)\n",
    "reverse_output_vocab =  dict((v, k) for k, v in output_vocab.items())\n",
    "\n",
    "K.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating KERAS model\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = models.Basic(test=2)\n",
    "BasicModelNetwork = model.build(vocab_size = len(output_vocab),\n",
    "                                embedding_size = embedding_size,\n",
    "                                hidden_size = 32,\n",
    "                                PADDING_SIZE = PADDING_SIZE,\n",
    "                                LEARNING_RATE = LEARNING_RATE,\n",
    "                                INPUT_DROPOUT = 0.2,\n",
    "                                LSTM_DROPOUT = 0.45,\n",
    "                                RECURRENT_DROPOUT = 0.35,\n",
    "                                N_EPOCHS = N_EPOCHS)\n",
    "\n",
    "if print_model is True:\n",
    "    BasicModelNetwork.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generators.Basic(batch_size = 64,\n",
    "                                training_file_path = training_file_path,\n",
    "                                gold_file_path = gold_file_path,\n",
    "                                antivocab = antivocab,\n",
    "                                output_vocab = output_vocab,\n",
    "                                PADDING_SIZE = PADDING_SIZE)\n",
    "\n",
    "validation_generator = generators.Basic(batch_size = 64,\n",
    "                                         training_file_path = training_file_path_dev,\n",
    "                                         gold_file_path = gold_file_path_dev,\n",
    "                                         antivocab = antivocab,\n",
    "                                         output_vocab = output_vocab,\n",
    "                                         PADDING_SIZE = PADDING_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../resources/logging'):\n",
    "    os.mkdir('../resources/logging')\n",
    "model_name = time.strftime('%Y-%m-%d_%H:%M:%S_%z')\n",
    "cbk = K.callbacks.TensorBoard('../resources/logging/keras_model_'+model_name)\n",
    "\n",
    "early_stopping = K.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=2, mode='auto')\n",
    "csv_logger = K.callbacks.CSVLogger('../resources/logging/keras_model_'+model_name+'.log')\n",
    "model_checkpoint = K.callbacks.ModelCheckpoint(filepath = '../resources/logging/keras_model_'+model_name+'.h5',\n",
    "                                               monitor='val_precision',\n",
    "                                               verbose=2,\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True,\n",
    "                                               mode='auto', period=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "from tqdm import tqdm\n",
    "c1, c2 = [0,0], [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\tloss: 11.449 \tacc: 0.653\tval_loss: 11.248 \tval_acc: 0.555:   0%|          | 0/4 [00:29<?, ?it/s]\n",
      "Epoch 2/4\tloss: 10.280 \tacc: 0.653\tval_loss: 9.580 \tval_acc: 0.555:   0%|          | 0/4 [00:20<?, ?it/s]\n",
      "Epoch 3/4\tloss: 7.508 \tacc: 0.653\tval_loss: 7.142 \tval_acc: 0.555:   0%|          | 0/4 [00:20<?, ?it/s]\n",
      "Epoch 4/4\tloss: 4.314 \tacc: 0.653\tval_loss: 4.697 \tval_acc: 0.555:   0%|          | 0/4 [00:29<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    with tqdm(total=epochs) as epoch_bar:\n",
    "        c1, c2 = [0,0], [0,0]\n",
    "        epoch_bar.set_description(\"Epoch {}/{}\\tloss: {:.3f} \\tacc: {:.3f}\\tval_loss: {:.3f} \\tval_acc: {:.3f}\".format(epoch, epochs, *c1, *c2))\n",
    "        for batch_x, batch_y in train_generator.__getitem__():\n",
    "            c1 = BasicModelNetwork.train_on_batch(batch_x, batch_y)\n",
    "            epoch_bar.set_description(\"Epoch {}/{}\\tloss: {:.3f} \\tacc: {:.3f}\\tval_loss: {:.3f} \\tval_acc: {:.3f}\".format(epoch, epochs, *c1, *c2))\n",
    "\n",
    "        for batch_x, batch_y in validation_generator.__getitem__():\n",
    "            c2 = BasicModelNetwork.test_on_batch(batch_x, batch_y)\n",
    "            epoch_bar.set_description(\"Epoch {}/{}\\tloss: {:.3f} \\tacc: {:.3f}\\tval_loss: {:.3f} \\tval_acc: {:.3f}\".format(epoch, epochs, *c1, *c2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\tloss: 2.383 \tacc: 0.653:   0%|          | 0/2 [00:12<?, ?it/s]\n",
      "Epoch 0/4\tval_loss: 4.267 \tval_acc: 0.493:   0%|          | 0/4 [00:05<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    with tqdm(total=len(train_generator)) as pbar:\n",
    "        for batch_x, batch_y in train_generator.__getitem__():\n",
    "            c1 = BasicModelNetwork.train_on_batch(batch_x, batch_y)\n",
    "            pbar.set_description(\"Epoch {}/{}\\tloss: {:.3f} \\tacc: {:.3f}\".format(epoch, epochs, *c1))\n",
    "\n",
    "    with tqdm(total=len(validation_generator)) as pbar:\n",
    "        for batch_x, batch_y in validation_generator.__getitem__():\n",
    "            c2 = BasicModelNetwork.test_on_batch(batch_x, batch_y)\n",
    "            pbar.set_description(\"Epoch {}/{}\\tval_loss: {:.3f} \\tval_acc: {:.3f}\".format(epoch, epochs, *c2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(train_generator)) as pbar:\n",
    "    \n",
    "    for batch_x, batch_y in train_generator.__getitem__():\n",
    "        c1 = BasicModelNetwork.train_on_batch(batch_x, batch_y)\n",
    "        pbar.set_description(\"val: {:.1f} \\tacc: {:.1f}\".format(*c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loss: {}, acc: {}\".format(*c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "lv = True\n",
    "ns = True\n",
    "for i in trange(2, desc='outer0 loop', leave=True):\n",
    "    for j in trange(4, desc='inner1 loop', leave=lv, position=ns):\n",
    "        for k in trange(100, desc='inner2 loop', leave=lv, position=ns):\n",
    "            sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "t = trange(train_generator.__getitem__(), desc='Bar desc', leave=True)\n",
    "for i in t:\n",
    "    t.set_description(\"Bar desc (file %i)\" % i)\n",
    "    sleep(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in t:\n",
    "    t.set_description(\"Bar desc (file %i)\" % i)\n",
    "    sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = (3 * n for n in range(length))  # just doing something random\n",
    "for n in tqdm(generator, total=length):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_generator.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (i, j) in iter(x):\n",
    "    print(i.shape, j.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    for j in range(len(data_y) // batchsize):\n",
    "        batch_x = data_x[i * batchsize: (i + 1) * batchsize]\n",
    "        batch_y = data_y[i * batchsize: (i + 1) * batchsize]\n",
    "        c1 = model.train_on_batch(batch_x, batch_y)\n",
    "        c2 = model.test_on_batch(batch_x, batch_y)\n",
    "        print('train', c1, '///', 'test', c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reverse_output_vocab[e] for e in i[0,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicModelNetwork.fit_generator(next(iter(train_generator)), \n",
    "                                steps_per_epoch=2,\n",
    "                                epochs=5, \n",
    "                                verbose=1,\n",
    "                                callbacks=[cbk, csv_logger, early_stopping],\n",
    "                                validation_data=next(iter(validation_generator)),\n",
    "                                validation_steps=2,\n",
    "                                class_weight=None,\n",
    "                                max_queue_size=10,\n",
    "                                workers=-1, \n",
    "                                use_multiprocessing=False,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicModelNetwork.fit_generator(train_generator.__getitem__, \n",
    "                                epochs=5, \n",
    "                                verbose=1,\n",
    "                                callbacks=[cbk, csv_logger, early_stopping],\n",
    "                                validation_data=validation_generator.__getitem__,\n",
    "                                class_weight=None,\n",
    "                                max_queue_size=10,\n",
    "                                workers=-1, \n",
    "                                use_multiprocessing=False,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.__getitem__().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicModelNetwork.fit_generator(train_generator.__getitem__(), \n",
    "                                steps_per_epoch=2,\n",
    "                                epochs=5, \n",
    "                                verbose=1,\n",
    "                                callbacks=[cbk, csv_logger, early_stopping],\n",
    "                                validation_data=validation_generator.__getitem__(),\n",
    "                                validation_steps=2,\n",
    "                                class_weight=None,\n",
    "                                max_queue_size=10,\n",
    "                                workers=-1, \n",
    "                                use_multiprocessing=False,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in train_generator:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0\n",
    "for x,y in tqdm(train_generator):\n",
    "    q+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_generator = model.prepare_sentence_batch(batch_size = batch_size,\n",
    "                                                training_file_path = training_file_path,\n",
    "                                                gold_file_path = gold_file_path,\n",
    "                                                antivocab = antivocab,\n",
    "                                                output_vocab = output_vocab,\n",
    "                                                PADDING_SIZE = PADDING_SIZE)\n",
    "\n",
    "\n",
    "for x,y in tqdm(train_generator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'testing.npy'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = model.prepare_sentence_batch(batch_size = 64,\n",
    "                               training_file_path = '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.xml',\n",
    "                               gold_file_path =  '../resources/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.txt',\n",
    "                               antivocab = antivocab,\n",
    "                               output_vocab = output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in batch:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y.reshape(*(y).shape,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for x, y in tqdm(batch):\n",
    "    if c==10:\n",
    "        break\n",
    "    c+=1\n",
    "    print(c, x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ones((3,4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(shuffle(x, y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in tqdm(validation_generator):\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n",
    "gold_file_path_dev = '../resources/WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = model.prepare_sentence_batch(batch_size = 64,\n",
    "                                                     training_file_path = training_file_path_dev,\n",
    "                                                     gold_file_path = gold_file_path_dev,\n",
    "                                                     antivocab = antivocab,\n",
    "                                                     output_vocab = output_vocab,\n",
    "                                                     PADDING_SIZE = PADDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in validation_generator:\n",
    "    print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in enumerate(i):\n",
    "    print(ind, \"\\t\", row[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(iter):\n",
    "    try:\n",
    "        return len(iter)\n",
    "    except TypeError:\n",
    "        return sum(1 for _ in iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "306//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_output_vocab[24].split(\":\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reverse_output_vocab[q] for q in i[2,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, concatenate, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "def data_gen(top_dim, bot_dim):\n",
    "    \"\"\"\n",
    "    Generator to yield inputs and their labels in batches.\n",
    "    \"\"\"\n",
    "    batch_size = 16\n",
    "    while True:\n",
    "        batch_imgs = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            # Create random arrays\n",
    "            rand_pix = np.random.randint(100, 256)\n",
    "            top_img = np.full(top_dim, rand_pix)\n",
    "            bot_img = np.full(bot_dim, rand_pix)\n",
    "\n",
    "            # Set a label\n",
    "            label = np.random.choice([0, 1])\n",
    "\n",
    "            batch_imgs.append([top_img, bot_img])\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        yield batch_imgs, batch_labels\n",
    "def data_gen(top_dim, bot_dim):\n",
    "    \"\"\"\n",
    "    Generator to yield batches of two inputs (per sample) with shapes top_dim and \n",
    "    bot_dim along with their labels.\n",
    "    \"\"\"\n",
    "    batch_size = 264\n",
    "    while True:\n",
    "        top_batch = []\n",
    "        bot_batch = []\n",
    "        batch_labels = []\n",
    "        for i in range(batch_size):\n",
    "            # Create random arrays\n",
    "            rand_pix = np.random.randint(100, 256)\n",
    "            top_img = np.full(top_dim, rand_pix)\n",
    "            bot_img = np.full(bot_dim, rand_pix)\n",
    "\n",
    "            # Set a label\n",
    "            label = np.random.choice([0, 1])\n",
    "            batch_labels.append(label)\n",
    "\n",
    "            # Pack each input image separately\n",
    "            top_batch.append(top_img)\n",
    "            bot_batch.append(bot_img)\n",
    "\n",
    "        yield [np.array(top_batch), np.array(bot_batch)], np.array(batch_labels)\n",
    "\n",
    "def get_compiled_model(top_dim, bot_dim):\n",
    "    \"\"\"\n",
    "    Return a two input one output model.\n",
    "    \"\"\"\n",
    "    # Get the top and bottom networks\n",
    "    top = get_part_model(top_dim)\n",
    "    bot = get_part_model(bot_dim)\n",
    "\n",
    "    # Prepare inputs and unify the top and the bottom networks\n",
    "    inp_top = Input(shape=top_dim)\n",
    "    inp_bot = Input(shape=bot_dim)\n",
    "    fusion = concatenate([top(inp_top), bot(inp_bot)])\n",
    "\n",
    "    predictions = Dense(1, activation='sigmoid')(fusion)\n",
    "\n",
    "    # Final full model\n",
    "    model = Model([inp_top, inp_bot], outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dim, bot_dim = (16, 16, 1), (16, 16, 3)\n",
    "model = get_compiled_model(top_dim, bot_dim)\n",
    "my_gen = data_gen(top_dim, bot_dim)\n",
    "model.fit_generator(my_gen, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
